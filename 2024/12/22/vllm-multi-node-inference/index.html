
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>用 vLLM 在多节点多卡上部署 Qwen2.5 以及进行推理 - Alpha Hinex&#39;s Blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Java, JavaScript, Spring, Html5, NoSQL, Docker, DevOps,vllm, gptq, gptq_marlin, tensor-parallel-size, Qwen2.5-32B-Instruct-GPTQ-Int4, multi-node inference, docker, nvidia container toolkit, max-model-len, gpu-memory-utilization, tesla t4"> 
    <meta name="description" content="本文记录了在两台机器，每台机器一块 Tesla T4 显卡的环境下，使用 vLLM 部署 Qwen2.5-32B-Instruct-GPTQ-Int4 模型的过程及遇到的问题，供类似环境使用 vLL,"> 
    <meta name="author" content="Alpha Hinex"> 

    <meta name="msvalidate.01" content="D769824B4D44C14A4C777A6EC4E898FC"> 
    <meta name="baidu-site-verification" content="TimiEK4Y9V"> 
    <meta name="google-site-verification" content="B-WME81HWSnMIkZZxcxv7bVI6yjbpAFKvifi2X-EkzQ"> 

    <link rel="alternative" href="atom.xml" title="Alpha Hinex&#39;s Blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link href="https://fonts.loli.net/css?family=Roboto+Mono|Rubik&display=swap" rel="stylesheet">
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/animate.css/3.7.2/animate.min.css">

    
<link rel="stylesheet" href="/css/share.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/theme/dracula.css">

    
<link rel="stylesheet" href="/css/obsidian.css">

    
<link rel="stylesheet" href="/css/ball-atom.min.css">

<meta name="generator" content="Hexo 4.2.1"></head>


<body class="loading">
    <div class="loader">
        <div class="la-ball-atom la-2x">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
    <span id="config-title" style="display:none">Alpha Hinex&#39;s Blog</span>
    <div id="loader"></div>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation animated fadeIn fast delay-1s">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="https://AlphaHinex.github.io">
        <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div>
        <h3 class="subtitle">用 vLLM 在多节点多卡上部署 Qwen2.5 以及进行推理</h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                        <a href="javascript:;" class="iconfont icon-share1"></a>
                        <div class="share-component-cc" data-disabled="facebook,douban,linkedin,diandian,tencent,google"></div>
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class=article-header-wrapper>
    <div class="article-header">
        <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/contents/covers/vllm-multi-node-inference.png);">
        </div>
        <div class="else">
            <p class="animated fadeInDown">
                
                <a href="/categories/AI"><b>「
                    </b>AI<b> 」</b></a>
                
                十二月 22, 2024
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/2024/12/22/vllm-multi-node-inference/" title="用 vLLM 在多节点多卡上部署 Qwen2.5 以及进行推理" class="">用 vLLM 在多节点多卡上部署 Qwen2.5 以及进行推理</a>
            </h3>
            
            <p class="post-count animated fadeInDown">
                
                <span>
                    <b class="iconfont icon-text2"></b> <i>文章字数</i>
                    14k
                </span>
                
                
                <span>
                    <b class="iconfont icon-timer__s"></b> <i>阅读约需</i>
                    13 mins.
                </span>
                
                
                
                <span id="busuanzi_page_pv_container" style="display: flex;">
                    <b class="iconfont icon-read"></b> <i>阅读次数</i>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                
            </p>
            
            
            <ul class="animated fadeInDown post-tags-list" itemprop="keywords"><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/vLLM/" rel="tag">vLLM</a></li></ul>
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn">
            <p>本文记录了在两台机器，每台机器一块 Tesla T4 显卡的环境下，使用 vLLM 部署 Qwen2.5-32B-Instruct-GPTQ-Int4 模型的过程及遇到的问题，供类似环境使用 vLLM 进行多节点多卡推理参考。</p>
<h1 id="部署清单"><a href="#部署清单" class="headerlink" title="部署清单"></a>部署清单</h1><ol>
<li><a href="https://modelscope.cn/models/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4/files" target="_blank" rel="noopener">Qwen2.5-32B-Instruct-GPTQ-Int4</a>、<a href="https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html" target="_blank" rel="noopener">vLLM</a></li>
<li><a href="https://download.docker.com/linux/static/stable/x86_64/docker-27.4.0.tgz" target="_blank" rel="noopener">docker v27.4.0</a>、<a href="https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.17.3" target="_blank" rel="noopener">nvidia-container-toolkit v1.17.3</a></li>
<li>Tesla T4 显卡驱动 <a href="https://cn.download.nvidia.com/tesla/550.127.08/NVIDIA-Linux-x86_64-550.127.08.run" target="_blank" rel="noopener">v550.127.08 CUDA12.4</a></li>
</ol>
<h2 id="部署包准备"><a href="#部署包准备" class="headerlink" title="部署包准备"></a>部署包准备</h2><pre><code class="bash"># qwen
$ git clone https://www.modelscope.cn/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4.git

# vllm image
$ docker pull vllm/vllm-openai:v0.6.4.post1

# export
$ docker save vllm/vllm-openai:v0.6.4.post1 | gzip &gt; images.tar.gz</code></pre>
<h1 id="更新显卡驱动"><a href="#更新显卡驱动" class="headerlink" title="更新显卡驱动"></a>更新显卡驱动</h1><p>需要更新至 cuda&gt;=12.4，以运行 vLLM 容器。</p>
<pre><code class="bash"># 先卸载之前安装的驱动 
$ sh ./NVIDIA-Linux-x86_64-550.127.08.run --uninstall 
# 再安装驱动 
$ sh ./NVIDIA-Linux-x86_64-550.127.08.run 
# 检测驱动 
$ nvidia-smi</code></pre>
<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><h2 id="Docker-Engine"><a href="#Docker-Engine" class="headerlink" title="Docker Engine"></a>Docker Engine</h2><pre><code class="bash">$ tar -xzf docker-27.4.0.tgz
$ cp docker/* /usr/local/bin/
$ docker -v</code></pre>
<p>将 <a href="https://github.com/containerd/containerd/blob/main/containerd.service" target="_blank" rel="noopener">https://github.com/containerd/containerd/blob/main/containerd.service</a> 内容保存至 <code>/usr/lib/systemd/system/containerd.service</code>：</p>
<pre><code class="service"># Copyright The containerd Authors.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target dbus.service

[Service]
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target</code></pre>
<pre><code class="bash">$ systemctl enable --now containerd
$ systemctl status containerd</code></pre>
<p>将下面内容保存至 <code>/usr/lib/systemd/system/docker.service</code>：</p>
<pre><code class="service">[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/local/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutStartSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
Delegate=yes
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target</code></pre>
<pre><code class="bash">$ systemctl enable --now docker
$ systemctl status docker</code></pre>
<h2 id="Nvidia-Container-Toolkit"><a href="#Nvidia-Container-Toolkit" class="headerlink" title="Nvidia Container Toolkit"></a>Nvidia Container Toolkit</h2><pre><code class="bash">$ tar -xzf nvidia-container-toolkit_1.17.3_rpm_x86_64.tar.gz
$ cd release-v1.17.3-stable/packages/centos7/x86_64
$ rpm -i libnvidia-container1-1.17.3-1.x86_64.rpm
$ rpm -i libnvidia-container-tools-1.17.3-1.x86_64.rpm
$ rpm -i nvidia-container-toolkit-base-1.17.3-1.x86_64.rpm
$ rpm -i nvidia-container-toolkit-1.17.3-1.x86_64.rpm 
# 检查安装情况
$ nvidia-ctk -h
# 配置 Nvidia Container Runtime
$ nvidia-ctk runtime configure --runtime=docker
# 检查配置
$ cat /etc/docker/daemon.json
# 重启 docker
$ systemctl restart docker
# 重启服务后执行如下命令查看效果：
$ docker info | grep Runtimes
 Runtimes: io.containerd.runc.v2 nvidia runc</code></pre>
<h1 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h1><h2 id="1-校验模型文件"><a href="#1-校验模型文件" class="headerlink" title="1. 校验模型文件"></a>1. 校验模型文件</h2><pre><code class="text">942d93a82fb6d0cb27c940329db971c1e55da78aed959b7a9ac23944363e8f47  model-00001-of-00005.safetensors
19139f34508cb30b78868db0f19ed23dbc9f248f1c5688e29000ed19b29a7eef  model-00002-of-00005.safetensors
d0f829efe1693dddaa4c6e42e867603f19d9cc71806df6e12b56cc3567927169  model-00003-of-00005.safetensors
3a5a428f449bc9eaf210f8c250bc48f3edeae027c4ef8ae48dd4f80e744dd19e  model-00004-of-00005.safetensors
c22a1d1079136e40e1d445dda1de9e3fe5bd5d3b08357c2eb052c5b71bf871fe  model-00005-of-00005.safetensors</code></pre>
<pre><code class="bash">$ cd /root/model/Qwen2.5-32B-Instruct-GPTQ-Int4
$ sha256sum *.safetensors &gt; sum.txt</code></pre>
<h2 id="2-配置集群"><a href="#2-配置集群" class="headerlink" title="2. 配置集群"></a>2. 配置集群</h2><p>在两台机器分别准备好 <code>vllm/vllm-openai:v0.6.4.post1</code> 镜像后，将 <a href="https://github.com/vllm-project/vllm/blob/main/examples/run_cluster.sh" target="_blank" rel="noopener">https://github.com/vllm-project/vllm/blob/main/examples/run_cluster.sh</a> 存放至 <code>/root/model/</code>：</p>
<pre><code class="sh">#!/bin/bash

# Check for minimum number of required arguments
if [ $# -lt 4 ]; then
    echo &quot;Usage: $0 docker_image head_node_address --head|--worker path_to_hf_home [additional_args...]&quot;
    exit 1
fi

# Assign the first three arguments and shift them away
DOCKER_IMAGE=&quot;$1&quot;
HEAD_NODE_ADDRESS=&quot;$2&quot;
NODE_TYPE=&quot;$3&quot;  # Should be --head or --worker
PATH_TO_HF_HOME=&quot;$4&quot;
shift 4

# Additional arguments are passed directly to the Docker command
ADDITIONAL_ARGS=(&quot;$@&quot;)

# Validate node type
if [ &quot;${NODE_TYPE}&quot; != &quot;--head&quot; ] &amp;&amp; [ &quot;${NODE_TYPE}&quot; != &quot;--worker&quot; ]; then
    echo &quot;Error: Node type must be --head or --worker&quot;
    exit 1
fi

# Define a function to cleanup on EXIT signal
cleanup() {
    docker stop node
    docker rm node
}
trap cleanup EXIT

# Command setup for head or worker node
RAY_START_CMD=&quot;ray start --block&quot;
if [ &quot;${NODE_TYPE}&quot; == &quot;--head&quot; ]; then
    RAY_START_CMD+=&quot; --head --port=6379&quot;
else
    RAY_START_CMD+=&quot; --address=${HEAD_NODE_ADDRESS}:6379&quot;
fi

# Run the docker command with the user specified parameters and additional arguments
docker run \
    --entrypoint /bin/bash \
    --network host \
    --name node \
    --shm-size 10.24g \
    --gpus all \
    -v &quot;${PATH_TO_HF_HOME}:/root/.cache/huggingface&quot; \
    &quot;${ADDITIONAL_ARGS[@]}&quot; \
    &quot;${DOCKER_IMAGE}&quot; -c &quot;${RAY_START_CMD}&quot;</code></pre>
<p>选择 节点1 作为 head node，节点2 作为 worker node。</p>
<p>在 节点1 执行：</p>
<pre><code class="bash">nohup bash run_cluster.sh \
    vllm/vllm-openai:v0.6.4.post1 \
    IP_OF_HEAD_NODE \
    --head \
    /root/model &gt; nohup.log 2&gt;&amp;1 &amp;</code></pre>
<p>在 节点2 执行：</p>
<pre><code class="bash">nohup bash run_cluster.sh \
    vllm/vllm-openai:v0.6.4.post1 \
    IP_OF_HEAD_NODE \
    --worker \
    /root/model &gt; nohup.log 2&gt;&amp;1 &amp;</code></pre>
<blockquote>
<p>注意：两个节点执行脚本指定的都是 head 节点的 IP。</p>
</blockquote>
<p>在任意节点通过 <code>docker exec -ti node bash</code> 进入容器：</p>
<pre><code class="bash"># 查看集群状态
$ ray status</code></pre>
<h2 id="3-启动-vLLM-服务"><a href="#3-启动-vLLM-服务" class="headerlink" title="3. 启动 vLLM 服务"></a>3. 启动 vLLM 服务</h2><p>在 节点1 的容器中启动服务（按当前显卡配置，GPU 利用率 90% 的前提下，只能将原始模型 32k 的上下文长度缩减到 4k）：</p>
<pre><code class="bash"># 根据 2 个节点和每个节点 1 个 GPU 设置总的 tensor-parallel-size
$ nohup vllm serve /root/.cache/huggingface/Qwen2.5-32B-Instruct-GPTQ-Int4 \
    --served-model-name Qwen2.5-32B-Instruct-GPTQ-Int4 \
    --tensor-parallel-size 2 --max-model-len 4096 \
    &gt; vllm_serve_qwen_nohup.log 2&gt;&amp;1 &amp;</code></pre>
<h3 id="参数调整过程"><a href="#参数调整过程" class="headerlink" title="参数调整过程"></a>参数调整过程</h3><p>默认 <code>gpu-memory-utilization</code>（<code>0.9</code>）时，日志中输出的 <code># GPU blocks</code> 为 <code>0</code>。</p>
<blockquote>
<p>No available memory for the cache blocks. Try increasing gpu_memory_utilization when initializing the engine. —— –gpu-memory-utilization 0.95</p>
</blockquote>
<p>调整 <code>gpu-memory-utilization</code> 为 <code>0.95</code> 后，<code># GPU blocks: 271</code>，<code>271 * 16 = 4336</code>，即下面报错中的 KV cache token 数。</p>
<blockquote>
<p>The model’s max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (4336). Try increasing gpu_memory_utilization or decreasing max_model_len when initializing the engine. —— –max_model_len 4096</p>
</blockquote>
<p>添加 <code>--max-model-len 4096</code> 后，<code># GPU blocks: 1548</code></p>
<h2 id="4-验证对话接口"><a href="#4-验证对话接口" class="headerlink" title="4. 验证对话接口"></a>4. 验证对话接口</h2><pre><code class="bash">curl --request POST \
  -H &quot;Content-Type: application/json&quot; \
  --url http://IP_OF_HEAD_NODE:8000/v1/chat/completions \
  --data &#39;{&quot;messages&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;我希望你充当 IT 专家。我会向您提供有关我的技术问题所需的所有信息，而您的职责是解决我的问题。你应该使用你的计算机科学、网络基础设施和 IT 安全知识来解决我的问题。在您的回答中使用适合所有级别的人的智能、简单和易于理解的语言将很有帮助。用要点逐步解释您的解决方案很有帮助。尽量避免过多的技术细节，但在必要时使用它们。我希望您回复解决方案，而不是写任何解释。我的第一个问题是“我的笔记本电脑出现蓝屏错误”。&quot;}],&quot;stream&quot;:true,&quot;model&quot;:&quot;Qwen2.5-32B-Instruct-GPTQ-Int4&quot;}&#39;</code></pre>
<blockquote>
<p>必须设置 <code>Content-Type</code> 请求头，否则会报 500 的错误：<a href="https://github.com/vllm-project/vllm/issues/11171" target="_blank" rel="noopener">[Bug]: Missing Content Type returns 500 Internal Server Error instead of 415 Unsupported Media Type</a></p>
</blockquote>
<h3 id="回复都是-！"><a href="#回复都是-！" class="headerlink" title="回复都是 ！"></a>回复都是 ！</h3><blockquote>
<p>we currently find two workarounds</p>
<ul>
<li>use gptq_marlin, which is available for Ampere and later cards.</li>
<li>change the number on this line from 50 to 0 and install from the modified source code. it may affect speed on short sequences though.<br>—— <a href="https://github.com/QwenLM/Qwen2.5/issues/1103#issuecomment-2507022590" target="_blank" rel="noopener">https://github.com/QwenLM/Qwen2.5/issues/1103#issuecomment-2507022590</a></li>
</ul>
</blockquote>
<p>目前 Qwen 和 vLLM 社区均向项目开发者报告了类似问题，<a href="https://github.com/jklj077" target="_blank" rel="noopener">jklj077</a> 暂时给出了两个绕过方案：</p>
<ol>
<li>需要修改模型文件中的 <code>config.json</code>，将其中的 <code>&quot;quant_method&quot;: &quot;gptq&quot;,</code> 修改为 <code>&quot;quant_method&quot;: &quot;gptq_marlin&quot;,</code>，但 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/quantization/utils/marlin_utils.py#L36" target="_blank" rel="noopener">需要显卡算力在 8.0 以上</a>；</li>
<li>需要修改 vLLM 源码，之后使用修改后源码安装。</li>
</ol>
<h2 id="5-验证补全接口"><a href="#5-验证补全接口" class="headerlink" title="5. 验证补全接口"></a>5. 验证补全接口</h2><pre><code class="bash">curl --request POST \
  -H &quot;Content-Type: application/json&quot; \
  --url http://IP_OF_HEAD_NODE:8000/v1/completions \
  --data &#39;{&quot;prompt&quot;:&quot;who r u?&quot;,&quot;model&quot;:&quot;Qwen2.5-32B-Instruct-GPTQ-Int4&quot;}&#39;</code></pre>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/12913261423" target="_blank" rel="noopener">nvidia显卡驱动安装</a></li>
<li><a href="https://blog.csdn.net/jianghuchuang/article/details/141220379" target="_blank" rel="noopener">Centos7.9离线安装Docker24(无坑版)_centos7.9 离线安装docker-CSDN博客</a></li>
<li><a href="https://alphahinex.github.io/2023/06/11/paddlenlp-codegen-copilot/">用 PaddleNLP 结合 CodeGen 实现离线 GitHub Copilot - Alpha Hinex’s Blog</a></li>
<li><a href="https://github.com/vllm-project/vllm/issues/10713" target="_blank" rel="noopener">[Usage]: vllm infer with 2 * Nvidia-L20, output repeat !!!!</a></li>
<li><a href="https://github.com/vllm-project/vllm/issues/10656" target="_blank" rel="noopener">[Bug]: Qwen2.5-32B-GPTQ-Int4 inference !!!!!</a></li>
<li><a href="https://docs.vllm.ai/en/stable/serving/distributed_serving.html" target="_blank" rel="noopener">Distributed Inference and Serving</a></li>
<li><a href="https://docs.vllm.ai/en/stable/serving/distributed_serving.html#multi-node-inference-and-serving" target="_blank" rel="noopener">vLLM - Multi-Node Inference and Serving</a></li>
<li><a href="https://blog.csdn.net/sunny0121/article/details/139331035" target="_blank" rel="noopener">大模型推理:vllm多机多卡分布式本地部署_vllm 多卡部署-CSDN博客</a></li>
<li><a href="https://www.xiaoiluo.com/article/vllm-gpu-ray-multigpu#google_vignette" target="_blank" rel="noopener">vLLM分布式多GPU Docker部署踩坑记 | LittleFish’Blog</a></li>
</ul>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls"
                data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
            <ul id="audio-list" style="display:none">
                
                
                <li title='0' data-url='/statics/background.mp3'></li>
                
                    
            </ul>
            
            
            
    <div id='gitalk-container' class="comment link"
        data-ae='true'
        data-ci='54f9966b8cc9d2423ffd'
        data-cs='504574e4532bdfa77d3e4091637ff53558408ac2'
        data-r='AlphaHinex.github.io'
        data-o='AlphaHinex'
        data-a='AlphaHinex'
        data-d=''
    >留言</div>


            
            
        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <img src="/img/hinex.jpg" height=300 width=300></img>
                    <p>Alpha Hinex</p>
                    <span>Stay Hungry. Stay Foolish.</span>
                    <dl>
                        <dd><a href="https://github.com/AlphaHinex" target="_blank"><span
                                    class=" iconfont icon-github"></span></a></dd>
                        <dd><a href="/whoami"><span class=" iconfont icon-wechat"></span></a></dd>
                        <dd><a href="" target="_blank"><span
                                    class=" iconfont icon-stack-overflow"></span></a></dd>
                    </dl>
                </div>
                <ul>
                    <li><a href="/">314 <p>文章</p></a></li>
                    <li><a href="/categories">76 <p>分类</p></a></li>
                    <li><a href="/tags">153 <p>标签</p></a></li>
                </ul>
            </div>
            
            
            
            <div class="box sticky animated fadeInRight faster">
                <div id="toc" class="subbox">
                    <h4>目录</h4>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#部署清单"><span class="toc-number">1.</span> <span class="toc-text">部署清单</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#部署包准备"><span class="toc-number">1.1.</span> <span class="toc-text">部署包准备</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#更新显卡驱动"><span class="toc-number">2.</span> <span class="toc-text">更新显卡驱动</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Docker"><span class="toc-number">3.</span> <span class="toc-text">Docker</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Docker-Engine"><span class="toc-number">3.1.</span> <span class="toc-text">Docker Engine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Nvidia-Container-Toolkit"><span class="toc-number">3.2.</span> <span class="toc-text">Nvidia Container Toolkit</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Qwen"><span class="toc-number">4.</span> <span class="toc-text">Qwen</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-校验模型文件"><span class="toc-number">4.1.</span> <span class="toc-text">1. 校验模型文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-配置集群"><span class="toc-number">4.2.</span> <span class="toc-text">2. 配置集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-启动-vLLM-服务"><span class="toc-number">4.3.</span> <span class="toc-text">3. 启动 vLLM 服务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#参数调整过程"><span class="toc-number">4.3.1.</span> <span class="toc-text">参数调整过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-验证对话接口"><span class="toc-number">4.4.</span> <span class="toc-text">4. 验证对话接口</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#回复都是-！"><span class="toc-number">4.4.1.</span> <span class="toc-text">回复都是 ！</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-验证补全接口"><span class="toc-number">4.5.</span> <span class="toc-text">5. 验证补全接口</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考资料"><span class="toc-number">5.</span> <span class="toc-text">参考资料</span></a></li></ol>
                </div>
            </div>
            
            
        </div>
    </div>
</div>

    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p>本站访客数<span id="busuanzi_value_site_uv"></span>人次 / 本站总访问量<span id="busuanzi_value_site_pv"></span>次</p>
    <p class="copyright" id="copyright">
        &copy; 2025
        <span class="gradient-text">
            Alpha Hinex
        </span>.
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
        Theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.3" target="_blank" rel="noopener">v1.4.3</a></small>
    </p>
</footer>

<script type="text/javascript" src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX", "TeX"],
      linebreaks: {
        automatic: true
      },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"]
      ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      noUndefined: {
        attributes: {
          mathcolor: "red",
          mathbackground: "#FFEEEE",
          mathsize: "90%"
        }
      },
      Macros: {
        href: "{}"
      }
    },
    messageStyle: "none"
  });
</script>
<script>
  function initialMathJax() {
    MathJax.Hub.Queue(function () {
      var all = MathJax.Hub.getAllJax(),
        i;
      // console.log(all);
      for (i = 0; i < all.length; i += 1) {
        console.log(all[i].SourceElement().parentNode)
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  }

  function reprocessMathJax() {
    if (typeof MathJax !== 'undefined') {
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    }
  }
</script>



    
<link rel="stylesheet" href="/css/gitalk.css">

    
<script src="/js/gitalk.min.js"></script>



<script src="/js/jquery-3.4.1.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/typed.js/2.0.10/typed.min.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/blueimp-md5/2.12.0/js/md5.min.js"></script>


<script src="/js/social-share.min.js"></script>


<script src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.js"></script>

    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/javascript/javascript.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/css/css.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/xml/xml.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/clike/clike.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/php/php.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/shell/shell.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/python/python.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/groovy/groovy.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/diff/diff.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/nginx/nginx.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/properties/properties.min.js"></script>




    
<script src="/js/busuanzi.min.js"></script>

    <script>
        $(document).ready(function () {
            if ($('span[id^="busuanzi_"]').length) {
                initialBusuanzi();
            }
        });
    </script>



<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-69084811-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-69084811-1');
    </script>





<script>
    function initialTyped () {
        var typedTextEl = $('.typed-text');
        if (typedTextEl && typedTextEl.length > 0) {
            var typed = new Typed('.typed-text', {
                strings: ["Stay Hungry. Stay Foolish.", "常与同好争高下，莫与傻子论短长"],
                typeSpeed: 90,
                loop: true,
                loopCount: Infinity,
                backSpeed: 20,
            });
        }
    }

    if ($('.article-header') && $('.article-header').length) {
        $(document).ready(function () {
            initialTyped();
        });
    }
</script>




</html>
