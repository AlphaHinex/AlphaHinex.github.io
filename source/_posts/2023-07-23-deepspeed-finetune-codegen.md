---
id: deepspeed-finetune-codegen
title: "è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç "
description: "ä½¿ç”¨ DeepSpeed å¾®è°ƒ CodeGen æ¨¡å‹å¹¶å¯åœ¨ FauxPiolot ä¸­ä½¿ç”¨"
date: 2023.07.23 10:26
categories:
    - AI
tags: [AI, VS Code]
keywords: GitHub Copilot, Copilot, FauxPilot, VS Code, CodeGen, Triton Inference Server, GPU, FasterTransformer
cover: /contents/fauxpilot/cover.png
---

åœ¨ [ç”¨ PaddleNLP ç»“åˆ CodeGen å®ç°ç¦»çº¿ GitHub Copilot](https://alphahinex.github.io/2023/06/11/paddlenlp-codegen-copilot/) å’Œ [GitHub Copilot å¼€æºæ›¿ä»£å“ â€”â€” FauxPilot](https://alphahinex.github.io/2023/06/18/fauxpilot/) ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ PaddleNLP å’Œ FauxPilot å°† CodeGen æ¨¡å‹ä»£ç†ä¸ºå¯é€šè¿‡ HTTP è¯·æ±‚è®¿é—®çš„æ¥å£ï¼Œå¹¶é€šè¿‡ VS Code æ’ä»¶åœ¨ IDE ä¸­è·å¾—ä¸ GitHub Copilot ç±»ä¼¼çš„ AI è¾…åŠ©ç¼–ç èƒ½åŠ›ã€‚

ä½†ä¸è®ºæ˜¯è¿™ç§æ–¹å¼ä¹Ÿå¥½ï¼Œæˆ–è€…æ˜¯ GitHub Copilotï¼Œèƒ½å¤Ÿè¾…åŠ©ç¼–å†™çš„éƒ½æ˜¯é€šç”¨ä»£ç ï¼Œæ— æ³•è¾…åŠ©ç¼–å†™å†…éƒ¨æ¡†æ¶æˆ–ç§æœ‰ç±»åº“çš„ç›¸å…³ä»£ç ã€‚

è¿™ä¸ªåœºæ™¯å¯ä»¥é€šè¿‡å¯¹ CodeGen æ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥å®ç°ã€‚

æœ¬æ–‡ä»‹ç»äº†åŸºäº [CodeGen-350M-multi](https://huggingface.co/Salesforce/codegen-350M-multi) æ¨¡å‹ï¼Œä½¿ç”¨ [DeepSpeed](https://www.deepspeed.ai/) å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨ [FauxPilot](https://github.com/fauxpilot/fauxpilot) é¡¹ç›®ä¸­æä¾›çš„è„šæœ¬ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè½¬æ¢ï¼Œä»¥ä½¿ç”¨ [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) è¿›è¡ŒåŠ é€Ÿï¼Œæœ€ç»ˆåœ¨ VS Code çš„ [FauxPilot](https://github.com/Venthe/vscode-fauxpilot) æ’ä»¶ä¸­ï¼Œå®ç°è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç çš„æ•ˆæœã€‚

# æ¨¡å‹å¾®è°ƒ

## DeepSpeed å¾®è°ƒç¯å¢ƒ

DeepSpeed ä¾èµ– [PyTorch](https://pytorch.org/)ï¼Œå®Œæ•´çš„ç¯å¢ƒéœ€æ±‚å¯è§å®˜æ–¹æ–‡æ¡£ [Requirements](https://github.com/microsoft/DeepSpeed#requirements)ï¼Œæœ¬æ–‡åœ¨ Docker é•œåƒä¸­æ‰§è¡Œå¾®è°ƒï¼Œä½¿ç”¨ [deepspeed/deepspeed:latest_torch111](https://hub.docker.com/layers/deepspeed/deepspeed/latest_torch111/images/sha256-7e594486a330c7c53be12fdc3c1b426f853a3dd1dc43d9ea1dcdf5cbc19150c4?context=explore) ä½œä¸ºåŸºç¡€é•œåƒï¼Œ[ğŸ¤— Transformers](https://github.com/huggingface/transformers) `v4.21.1` ç‰ˆæœ¬ä¸­çš„ [run_clm.py](https://github.com/huggingface/transformers/blob/v4.21.1/examples/pytorch/language-modeling/run_clm.py) è„šæœ¬ä½œä¸ºå¾®è°ƒè„šæœ¬ï¼Œéœ€åœ¨å¾®è°ƒç¯å¢ƒä¸­å®‰è£…å¾®è°ƒè„šæœ¬æ‰€éœ€ä¾èµ– [requirements.txt](https://github.com/huggingface/transformers/blob/v4.21.1/examples/pytorch/language-modeling/requirements.txt) åŠ `aiohttp` å’Œ `transformers`ã€‚

è¿™é‡Œéœ€æ³¨æ„ `run_clm.py` å’Œ `requirements.txt` è¦ä½¿ç”¨ä¸å®‰è£…çš„ Transformers ç‰ˆæœ¬ä¸€è‡´çš„æºç  tag ä¸­çš„æ–‡ä»¶ï¼Œå¦‚ä¸Šé¢é“¾æ¥å‡ä¸º `v4.21.1` ç‰ˆæœ¬çš„ã€‚

å¯å‚ç…§å¦‚ä¸‹ `Dockerfile` æ„å»ºå¾®è°ƒç¯å¢ƒæ‰€ä½¿ç”¨çš„é•œåƒï¼š

```Dockerfile
FROM deepspeed/deepspeed:latest_torch111

COPY requirements.txt requirements.txt

RUN pip install -r requirements.txt
RUN pip install aiohttp==3.6
RUN pip install transformers==4.21.1
```

æ„å»ºé•œåƒï¼š

```bash
docker build -t deepspeed:codegen .
```

ä½¿ç”¨é•œåƒå¯åŠ¨å¹¶è¿›å…¥å®¹å™¨ï¼š

```bash
$ docker run --name dstest --runtime=nvidia -v $PWD:/mnt -it deepspeed:codegen /bin/bash

=============
== PyTorch ==
=============

NVIDIA Release 21.12 (build 29870972)
PyTorch Version 1.11.0a0+b6df043

Container image Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

Copyright (c) 2014-2021 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: MOFED driver for multi-node communication was not detected.
      Multi-node communication performance may be reduced.

NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be
   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:
   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...
root@f8338550c41f:/workspace#
```

å®¹å™¨ä¸­ä½¿ç”¨ `ds_report` éªŒè¯ DeepSpeed çŠ¶æ€ï¼š

```bash
root@f8338550c41f:/workspace# ds_report
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [OKAY]
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [NO] ....... [OKAY]
cpu_adagrad ............ [NO] ....... [OKAY]
fused_adam ............. [NO] ....... [OKAY]
fused_lamb ............. [NO] ....... [OKAY]
 [WARNING]  please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [NO] ....... [NO]
transformer ............ [NO] ....... [OKAY]
stochastic_transformer . [NO] ....... [OKAY]
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [NO] ....... [NO]
utils .................. [NO] ....... [OKAY]
quantizer .............. [NO] ....... [OKAY]
transformer_inference .. [NO] ....... [OKAY]
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/opt/conda/lib/python3.8/site-packages/torch']
torch version .................... 1.11.0a0+b6df043
torch cuda version ............... 11.5
torch hip version ................ None
nvcc version ..................... 11.5
deepspeed install path ........... ['/opt/conda/lib/python3.8/site-packages/deepspeed']
deepspeed info ................... 0.6.5, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.5
```

å®¹å™¨ä¸­éªŒè¯ CUDA çŠ¶æ€ï¼š

```bash
root@f8338550c41f:/workspace# python
Python 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> torch.cuda.is_available()
True
```

## æ•°æ®é›†

å¾®è°ƒæ•°æ®é›†è‹¥ä½¿ç”¨ Hugging Face ä¸Šçš„ï¼Œå¯ç›´æ¥åœ¨å¾®è°ƒå‘½ä»¤ä¸­ä¼ å…¥æ•°æ®é›†åç§°ï¼Œå¦‚ [moyjx/debian_csrc](https://huggingface.co/datasets/moyix/debian_csrc)ã€‚å¦‚éœ€ä½¿ç”¨æœ¬åœ°æ•°æ®é›†ï¼Œä»…æ”¯æŒ `csv/json/txt` æ ¼å¼ï¼Œè¿™é‡Œçš„ `json` æ ¼å¼ï¼Œæ˜¯æŒ‡ [å¤„ç†å¤§æ•°æ®é›†çš„çµæ´»æ ¼å¼ â€”â€” JSON Lines](https://alphahinex.github.io/2023/07/16/json-lines/) ä¸­æåˆ°çš„ JSON Lines æ ¼å¼ï¼Œä¾‹å¦‚ï¼š

```jsonl
{"text": "content_of_source_file_1", "url": "path_to_source_file_1"}
{"text": "content_of_source_file_2", "url": "path_to_source_file_2"}
...
```

å…¶ä¸­ï¼š

- `text` å±æ€§ä¸ºå¿…éœ€å±æ€§ï¼Œä¿å­˜è®­ç»ƒæ•°æ®ï¼Œå³æºç æ–‡ä»¶å†…å®¹ï¼Œè¯¥å±æ€§åå¯åœ¨ `run_clm.py` è„šæœ¬ä¸­ä¿®æ”¹ã€‚
- å…¶ä»–å±æ€§å¯è‡ªæ„¿æ·»åŠ ï¼Œå¦‚ä¸Šé¢çš„ `url` å±æ€§å¯ä»¥æ ‡è¯†æ–‡ä»¶æ¥æºã€‚

å¯ä½¿ç”¨ [files2jsonl](https://github.com/AlphaHinex/go-toolkit/tree/main/files2jsonl) å·¥å…·å°†æºç æ–‡ä»¶å¤¹è½¬æ¢ä¸ºå¯ç›´æ¥ä½¿ç”¨çš„æœ¬åœ°æ•°æ®é›†ã€‚

## å¾®è°ƒå‘½ä»¤

```bash
deepspeed --num_gpus 4 --num_nodes 1 \
./run_clm.py \
--model_name_or_path=./codegen-350M-multi \
--per_device_train_batch_size=1 \
--learning_rate 2e-5 \
--num_train_epochs 1 \
--output_dir=./codegen-350M-multi-finetune \
--train_file ./test_dataset.json \
--tokenizer_name ./codegen-350M-multi \
--block_size 2048 \
--gradient_accumulation_steps 32 \
--do_train \
--fp16 \
--overwrite_output_dir \
--deepspeed ./ds_config.json
```

`train_file` å‚æ•°æŒ‡å®šæœ¬åœ°æ–‡ä»¶ã€‚è‹¥è¦ä½¿ç”¨ Hugging Face ä¸Šæ•°æ®é›†å¾®è°ƒï¼Œå¯ä½¿ç”¨ `dataset_name` å‚æ•°æŒ‡å®šæ•°æ®é›†åç§°ã€‚

`ds_config.json` å¯ä½¿ç”¨ [è¿™é‡Œçš„ç¤ºä¾‹](https://github.com/fauxpilot/fauxpilot/issues/62#issuecomment-1304681430):

```json
{
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 2000,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "wall_clock_breakdown": false
}
```

### ä½¿ç”¨å¤šå¡å¾®è°ƒ

`num_gpus` å‚æ•°å¯ä»¥æŒ‡å®šä½¿ç”¨çš„ GPU æ•°é‡ã€‚

å¦‚ä½¿ç”¨å¤šä¸ª GPU æ—¶é‡ä»¥ä¸‹æŠ¥é”™ï¼š

```text
Pytorch "NCCL error": unhandled system error, NCCL version 2.4.8"
```

å¯å‚ç…§ [è¿™é‡Œ](https://stackoverflow.com/questions/61075390/pytorch-nccl-error-unhandled-system-error-nccl-version-2-4-8#) åœ¨ `run_clm.py` ä¸­åŠ å…¥ `INFO` çº§åˆ«è°ƒè¯•ä¿¡æ¯ï¼Œå¦‚ï¼š

```diff
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version

+os.environ["NCCL_DEBUG"] = "INFO"

 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.21.0")
```

æŸ¥çœ‹è¯¦ç»†æŠ¥é”™ä¿¡æ¯ã€‚

å¦‚çœ‹åˆ°å…·ä½“ [æŠ¥é”™](https://github.com/NVIDIA/nccl/issues/290) ä¸ºï¼š

```text
NCCL WARN Call to posix_fallocate failed : No space left on device
```

å¯å‚ç…§ PaddlePaddle çš„ [è§£å†³æ–¹å¼](https://github.com/PaddlePaddle/Paddle/pull/28484/files)ï¼Œåœ¨ `run_clm.py` ä¸­åŠ å…¥ï¼š

```diff
 from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version

 os.environ["NCCL_DEBUG"] = "INFO"
+os.environ['NCCL_SHM_DISABLE'] = str(1)

 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.21.0")
```

### æŒ‡å®š GPU

è¦æŒ‡å®š GPU æ—¶ï¼Œå¯å‚ç…§ [deepspeedå¤šæœºå¤šå¡è®­ç»ƒè¸è¿‡çš„å‘](https://zhuanlan.zhihu.com/p/624223085) ä¸­å†…å®¹ï¼Œå»æ‰ `num_gpus` å’Œ `num_nodes` å‚æ•°ï¼Œä½¿ç”¨ `--include localhost:1,2` å½¢å¼é…ç½®å•æœºå¤šå¡ã€‚

## å¾®è°ƒè€—æ—¶è¯„ä¼°

ä½¿ç”¨ä¸€ä¸ª Tesla P40ï¼ˆ24G VRAMï¼‰å¾®è°ƒ CodeGen-350M-multi æ¨¡å‹ï¼Œæ˜¾å­˜ä½¿ç”¨ 23G å·¦å³ï¼Œå¾®è°ƒæ—¶é—´ï¼š

1. 40w è¡Œé‚®ç®±æ•°æ®ï¼Œ24M è®­ç»ƒæ•°æ®é›†ï¼Œå¤§çº¦è€—æ—¶ 10 åˆ†é’Ÿ
2. 300 ä¸ª java æ–‡ä»¶ï¼Œ75M è®­ç»ƒæ•°æ®é›†ï¼Œå¤§çº¦è€—æ—¶ 1 å°æ—¶ 20 åˆ†é’Ÿ

åœ¨ https://github.com/fauxpilot/fauxpilot/discussions/74#discussioncomment-3798458 ä¸­ï¼ŒFauxPilot ä½œè€…ä¹Ÿç»™å‡ºäº†ä»–ä»¬å¾®è°ƒ 16B æ¨¡å‹çš„èµ„æºéœ€æ±‚æƒ…å†µï¼š

> As a warning, fine-tuning or training large models (like CodeGen 16B) takes a lot of GPU resources â€“ we fine-tuned a 16B model on Verilog code, and it took 3xA100 GPUs with 80GB of VRAM each running for six days to do one pass over the 400MB dataset.

## å¾®è°ƒåéªŒè¯

æ¨¡å‹å¾®è°ƒä¹‹åï¼Œå¯é€šè¿‡å¦‚ä¸‹ Python ä»£ç è¿›è¡ŒéªŒè¯ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("/path/to/codegen-350M-multi-finetune")
model = AutoModelForCausalLM.from_pretrained("/path/to/codegen-350M-multi-finetune")

text = "def qucik_sort"
input_ids = tokenizer(text, return_tensors="pt").input_ids

generated_ids = model.generate(input_ids, max_length=128)
print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))
```

åœ¨ `text` ä¸­æ”¾å…¥æç¤ºè¯ï¼Œè§‚å¯Ÿ `print` è¾“å‡ºçš„ç»“æœï¼Œæ˜¯å¦å­¦ä¹ åˆ°äº†è®­ç»ƒæ•°æ®ä¸­çš„å†…å®¹ã€‚

# æ¨¡å‹è½¬æ¢

åœ¨é€šè¿‡ä¸Šé¢çš„ Python ä»£ç éªŒè¯å¾®è°ƒåçš„æ¨¡å‹èƒ½åŠ›æ—¶ï¼Œå¯ä»¥æ„Ÿå—åˆ°éœ€è¦çš„æ—¶é—´è¿˜æ˜¯å¾ˆé•¿çš„ï¼Œè¿™ä¸ªæ—¶é—´é•¿åˆ°æ— æ³•æ»¡è¶³åœ¨ IDE ä¸­å³æ—¶è¡¥å…¨ä»£ç çš„éœ€æ±‚ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒFauxPilot çš„ä½œè€…ä½¿ç”¨äº† [çº¿æ€§ä»£æ•°çš„æ–¹æ³•](https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566)ï¼Œé€šè¿‡ [gist ä¸Šçš„ codegen_gptj_convert.py](https://gist.github.com/moyix/0f37da9c21c4ddfa0ab39ddad1639db4) æˆ– [ä»“åº“ä¸­çš„ codegen_gptj_convert.py](https://github.com/fauxpilot/fauxpilot/blob/main/converter/codegen_gptj_convert.py) è½¬æ¢è„šæœ¬ï¼Œå°† CodeGen æ¨¡å‹è½¬æ¢ä¸ºäº† [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax#gpt-j-6b) æ¨¡å‹ã€‚

ä¹‹æ‰€ä»¥è½¬æ¢æˆ GPT-J æ¨¡å‹ï¼Œæ˜¯å› ä¸ºè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ¶æ„ä¸Šæœ‰ 99.9% çš„ç›¸ä¼¼ï¼Œå¹¶ä¸” GPT-J åœ¨æ¨ç†åŠ é€Ÿå¼•æ“ [FasterTransformer](https://github.com/NVIDIA/FasterTransformer/) çš„ [æ”¯æŒåˆ—è¡¨](https://github.com/NVIDIA/FasterTransformer/#support-matrix) ä¸­ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬ä¼šå‘ç°åœ¨ä½¿ç”¨ FauxPilot æ—¶ï¼Œæ˜¯å»ä½œè€…è‡ªå·±çš„ Hugging Face æ¨¡å‹ä»“åº“ä¸­ä¸‹è½½è½¬æ¢åçš„æ¨¡å‹ï¼ˆå¦‚ https://huggingface.co/moyix/codegen-350M-multi-gptj ï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ Salesforce å‘å¸ƒçš„åŸå§‹æ¨¡å‹çš„åŸå› ã€‚

åŸå§‹çš„ CodeGen æ¨¡å‹éœ€è¦ 12 ç§’ç”Ÿæˆ 128 ä¸ª tokenï¼Œç»è¿‡æ¨ç†åŠ é€Ÿåï¼Œåœ¨ä¸€ä¸ª A6000 GPU ä¸Šå¯ä»¥å°†è€—æ—¶ç¼©çŸ­åˆ° 5.7 ç§’ï¼Œå¹¶ä¸”ä½¿ç”¨å¤š GPU è¿˜æœ‰è¿›ä¸€æ­¥åŠ é€Ÿçš„å¯èƒ½ã€‚

å¯é€šè¿‡å¦‚ä¸‹æ­¥éª¤ï¼Œå°†æˆ‘ä»¬å¾®è°ƒå¥½çš„ CodeGen æ¨¡å‹ï¼Œè½¬æ¢ä¸ºå¯åœ¨ FauxPilot Server ä¸­ä½¿ç”¨çš„å½¢å¼ã€‚

## codegen_gptj_convert.py

å…ˆä½¿ç”¨ [codegen_gptj_convert.py](https://github.com/fauxpilot/fauxpilot/blob/main/converter/codegen_gptj_convert.py) è„šæœ¬ï¼Œå°† Salesforce CodeGen æ¨¡å‹è½¬æ¢ä¸º GPT-J æ¨¡å‹ã€‚

è½¬æ¢æœ¬åœ°å¾®è°ƒåçš„æ¨¡å‹æ—¶ï¼Œéœ€ä¿®æ”¹è„šæœ¬å†…å®¹ï¼Œå»æ‰ `choices=CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST, default='Salesforce/codegen-350M-multi',` è¡Œï¼š

```diff
 parser.add_argument('--code_model',
-                    choices=CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST, default='Salesforce/codegen-350M-multi',
                     help='which SalesForce model to convert'
                     )
```

ä½¿ç”¨ä¸‹é¢å‘½ä»¤æ‰§è¡Œè½¬æ¢ï¼š

```bash
python /path/to/codegen_gptj_convert.py \
--code_model /path/to/codegen-350M-multi-finetune \
/path/to/codegen-350M-multi-finetune-gptj
```

è½¬æ¢æ—¶éœ€è¦ `code_model` è·¯å¾„å†…çš„ `pytorch_model.bin` å’Œ `config.json` æ–‡ä»¶ï¼Œè½¬æ¢åæ¨¡å‹ä»ä¸ºä¸€ä¸ª `pytorch_model.bin` æ–‡ä»¶ï¼Œä½†å†…å®¹å‘ç”Ÿäº†å˜åŒ–ï¼Œé…å¥—çš„ `config.json` æ–‡ä»¶ä¹Ÿä¸ä¸€æ ·äº†ã€‚

> è„šæœ¬ç”¨æ³•å¯å‚ç…§ [download_and_convert_model.sh](https://github.com/fauxpilot/fauxpilot/blob/main/converter/download_and_convert_model.sh)ã€‚

## triton_config_gen.py

è½¬æ¢åçš„ GPT-J æ¨¡å‹åœ¨ç»è¿‡ FasterTransformer åŠ é€Ÿåï¼Œæœ€ç»ˆä¼šéƒ¨ç½²åˆ° [Triton Inference Server](https://github.com/triton-inference-server/backend) ä¸­ã€‚éœ€å…ˆä½¿ç”¨ [triton_config_gen.py](https://github.com/fauxpilot/fauxpilot/blob/main/converter/triton_config_gen.py) è„šæœ¬æ¥ç”Ÿæˆ Triton éœ€ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ã€‚

ä½†åœ¨ä½¿ç”¨ FauxPilot ä»“åº“ä¸­çš„è¿™ä¸ªè„šæœ¬ç”Ÿæˆ CodeGen-350M-multi å¾®è°ƒåæ¨¡å‹çš„é…ç½®æ—¶ï¼Œ`vocab_size` çš„ç®—æ³•éœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œå¦åˆ™ä½¿ç”¨è½¬æ¢åçš„æ¨¡å‹æ—¶ä¼šå‡ºç°è¡¥å…¨çš„éƒ½æ˜¯æ··ä¹±å†…å®¹çš„æƒ…å†µï¼š

```diff
 # Vocab size *sometimes* gets rounded up to a multiple of 1024
-params['vocab_size'] = tokenizer.vocab_size+len(tokenizer.get_added_vocab())  # round_up(tokenizer.vocab_size, 1024)
+params['vocab_size'] = round_up(tokenizer.vocab_size, 1024)
 params['start_id'] = tokenizer.eos_token_id
```

è°ƒæ•´è„šæœ¬åæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ç”Ÿæˆé…ç½®ï¼š

```bash
python /path/to/triton_config_gen.py -n 2 \
--tokenizer /path/to/codegen-350M-multi-finetune \
--hf_model_dir /path/to/codegen-350M-multi-finetune-gptj \
--model_store /path/to/fauxpilot/models \
--rebase /model
```

> `triton_config_gen.py` è„šæœ¬éœ€ä¸ [config_template.pbtxt](https://github.com/fauxpilot/fauxpilot/blob/main/converter/config_template.pbtxt) æ¨¡æ¿æ–‡ä»¶æ”¾åœ¨ç›¸åŒè·¯å¾„ä¸‹å…±åŒä½¿ç”¨ã€‚

å…¶ä¸­ï¼š

- `-n` ä¸ºæœ€ç»ˆè¿è¡Œæ—¶éœ€è¦ä½¿ç”¨çš„ GPU æ•°é‡
- `--tokenizer` æŒ‡å®šå¾®è°ƒåçš„ CodeGen æ¨¡å‹è·¯å¾„ï¼ˆå› ä¸ºä½¿ç”¨ `codegen_gptj_convert.py` è„šæœ¬è½¬æ¢å¾—åˆ°çš„ GPT-J æ¨¡å‹è·¯å¾„ä¸­åªæœ‰ `pytorch_model.bin` å’Œ `config.json` ä¸¤ä¸ªæ–‡ä»¶ï¼‰
- `--hf_model_dir` æŒ‡å®šè½¬æ¢åçš„ GPT-J æ¨¡å‹è·¯å¾„
- `--model_store` æŒ‡å®šé…ç½®æ–‡ä»¶çš„ç”Ÿæˆè·¯å¾„
- `--rebase` ç”¨æ¥æŒ‡å®šå°† FasterTransformer åŠ é€Ÿåçš„æ¨¡å‹æ–‡ä»¶æŒ‚è½½åˆ°å®¹å™¨é‡Œæ—¶ï¼Œå®¹å™¨å†…æ‰€ä½¿ç”¨çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚å¦‚ä½¿ç”¨ FauxPilot æä¾›çš„ Docker Compose æ–¹å¼å¯åŠ¨ FauxPilot Server æœåŠ¡ï¼Œå¯ä¿æŒä½¿ç”¨ `/model` è·¯å¾„ä¸å˜

ä»¥ä¸Šé¢çš„å‘½ä»¤ä¸ºä¾‹ï¼Œæ‰§è¡ŒæˆåŠŸåä¼šåœ¨ `/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer` è·¯å¾„ä¸‹ç”Ÿæˆä¸€ä¸ª `config.pbtxt` æ–‡ä»¶ã€‚

## huggingface_gptj_convert.py

ä½¿ç”¨ [huggingface_gptj_convert.py](https://github.com/fauxpilot/fauxpilot/blob/main/converter/huggingface_gptj_convert.py) è„šæœ¬å°† GPT-J æ¨¡å‹è½¬æ¢æˆ FasterTransformer æ ¼å¼ï¼š

```bash
python /path/to/huggingface_gptj_convert.py \
-in_file /path/to/codegen-350M-multi-finetune-gptj \
-saved_dir /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1 \
-infer_gpu_num 2
```

å…¶ä¸­ï¼š

- `in_file` ä¸ºè½¬æ¢æˆ GPT-J æ ¼å¼çš„å¾®è°ƒåæ¨¡å‹æ–‡ä»¶è·¯å¾„
- `saved_dir` ä¸ºä¸Šé¢ `triton_config_gen.py` è„šæœ¬ç”Ÿæˆé…ç½®æ–‡ä»¶çš„è·¯å¾„åŠ ä¸€å±‚ `/1`
- `infer_gpu_num` ä¸ºæ¨ç†æ‰€ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œæ³¨æ„éœ€ä¸ `triton_config_gen.py` è„šæœ¬çš„ `-n` å‚æ•°å€¼ä¸€è‡´

## All in one è„šæœ¬

å¯ä½¿ç”¨ [convert_model.sh](https://github.com/AlphaHinex/fauxpilot/blob/350m/converter/convert_model.sh) è„šæœ¬å®Œæˆä¸Šè¿°æ‰€æœ‰è½¬æ¢å·¥ä½œï¼Œç”¨æ³•ä¸ºï¼š

```bash
./convert_model.sh codegen-350M-multi-finetune 2
```

å°†å¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„æ”¾è‡³è¯¥è„šæœ¬è·¯å¾„å†…ï¼Œå¹¶å°†è¯¥è„šæœ¬ä¸å…¶ä»–è½¬æ¢æ‰€éœ€è„šæœ¬å’Œæ¨¡æ¿æ–‡ä»¶æ”¾ç½®åœ¨ç›¸åŒè·¯å¾„ä¸‹ã€‚ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºå¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œç¬¬äºŒä¸ªå‚æ•°ä¸ºæ¨ç†æ—¶éœ€ä½¿ç”¨çš„ GPU æ•°é‡ã€‚

è¯¥è„šæœ¬å†…å®¹å¦‚ä¸‹ï¼š

```bash
#!/bin/bash

MODEL=${1}
NUM_GPUS=${2}

echo "Converting model ${MODEL} with ${NUM_GPUS} GPUs"

python3 codegen_gptj_convert.py --code_model ./${MODEL} ${MODEL}-gptj

rm -rf ./models/${MODEL}-${NUM_GPUS}gpu

python3 triton_config_gen.py -n ${NUM_GPUS} --tokenizer ./${MODEL} --hf_model_dir ${MODEL}-gptj --model_store ./models --rebase /model

python3 huggingface_gptj_convert.py -in_file ${MODEL}-gptj -saved_dir ./models/${MODEL}-gptj-${NUM_GPUS}gpu/fastertransformer/1 -infer_gpu_num ${NUM_GPUS}

rm -rf ${MODEL}-gptj
```

æ‰§è¡ŒæˆåŠŸåï¼Œä¼šåœ¨è„šæœ¬æ‰€åœ¨ä½ç½®çš„ `models/codegen-350M-multi-finetune-gptj-2gpu` ä¸‹è·å¾—è½¬æ¢å¥½çš„æ¨¡å‹æ–‡ä»¶ã€‚

## æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶

å®é™…ä½¿ç”¨æ—¶å‘ç°ï¼Œç»è¿‡ä¸Šè¿°è¿‡ç¨‹è½¬æ¢åçš„æ¨¡å‹åœ¨ FauxPilot Server ä¸­ä½¿ç”¨æ—¶ï¼Œä¼šå‡ºç°è¡¥å…¨çš„ä»£ç å†…å®¹éƒ½æ˜¯æ··ä¹±çš„æ— æ³•è¾¨è¯†å†…å®¹ï¼Œç»è¯•éªŒå‘ç°éœ€è¦ä½¿ç”¨ FauxPilot ä½¿ç”¨çš„åŸå§‹æ¨¡å‹ä¸­çš„éƒ¨åˆ†æ–‡ä»¶æ›¿æ¢é€šè¿‡ä¸Šè¿°æ–¹å¼è½¬æ¢ä¹‹åçš„ FasterTransformer æ¨¡å‹æ–‡ä»¶ã€‚ä»¥ `CodeGen-350M-multi` ä¸ºä¾‹ï¼Œéœ€æ›¿æ¢çš„æ–‡ä»¶ä¸ºï¼š

```text
model.lm_head.bias.bin
model.lm_head.weight.bin
model.wte.bin
```

å¯åœ¨ https://huggingface.co/moyix/codegen-350M-multi-gptj/tree/main å¯¹åº” GPU æ•°é‡çš„ `zst` å‹ç¼©æ–‡ä»¶ä¸­ï¼Œæå–ä¸Šè¿°æ–‡ä»¶ï¼ˆæˆ–ä½¿ç”¨ Salesforce åŸå§‹æ¨¡å‹é€šè¿‡ä¸Šè¿°è¿‡ç¨‹è½¬æ¢å¾—åˆ°ï¼Œä¸å¾®è°ƒç›´æ¥è½¬æ¢æ—¶è¿™ä¸‰ä¸ªæ–‡ä»¶å†…å®¹åº”è¯¥æ˜¯æ­£ç¡®çš„ï¼Œå¯ä»¥åœ¨ FauxPilot Server ä¸­æ­£å¸¸ä½¿ç”¨ï¼‰ï¼Œå¹¶è¦†ç›–è‡ªè¡Œè½¬æ¢å‡ºçš„æ–‡ä»¶ï¼Œå¦‚ï¼š


```bash
cp /path/to/origin/codegen-350M-multi-2gpu/fastertransformer/1/2-gpu/model.lm_head.bias.bin /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1/2-gpu/model.lm_head.bias.bin
cp /path/to/origin/codegen-350M-multi-2gpu/fastertransformer/1/2-gpu/model.lm_head.weight.bin /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1/2-gpu/model.lm_head.weight.bin
cp /path/to/origin/codegen-350M-multi-2gpu/fastertransformer/1/2-gpu/model.wte.bin /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1/2-gpu/model.wte.bin
```

## æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘

```bash
$ pwd
/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu
$ tree -L 4
.
â””â”€â”€ fastertransformer
    â”œâ”€â”€ 1
    â”‚Â Â  â””â”€â”€ 2-gpu
    â”‚Â Â      â”œâ”€â”€ config.ini
    â”‚Â Â      â”œâ”€â”€ model.final_layernorm.bias.bin
    â”‚Â Â      â”œâ”€â”€ model.final_layernorm.weight.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.attention.dense.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.attention.query_key_value.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.input_layernorm.bias.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.input_layernorm.weight.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.mlp.dense_4h_to_h.bias.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.mlp.dense_4h_to_h.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.mlp.dense_h_to_4h.bias.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.0.mlp.dense_h_to_4h.weight.0.bin
...
    â”‚Â Â      â”œâ”€â”€ model.layers.9.attention.dense.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.attention.query_key_value.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.input_layernorm.bias.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.input_layernorm.weight.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.mlp.dense_4h_to_h.bias.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.mlp.dense_4h_to_h.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.mlp.dense_h_to_4h.bias.0.bin
    â”‚Â Â      â”œâ”€â”€ model.layers.9.mlp.dense_h_to_4h.weight.0.bin
    â”‚Â Â      â”œâ”€â”€ model.lm_head.bias.bin
    â”‚Â Â      â”œâ”€â”€ model.lm_head.weight.bin
    â”‚Â Â      â””â”€â”€ model.wte.bin
    â””â”€â”€ config.pbtxt
```

# æ–°æ¨¡å‹ä½¿ç”¨

åœ¨ FauxPilot ä¸­ä½¿ç”¨å¾®è°ƒå¹¶è½¬æ¢åçš„æ–°æ¨¡å‹å°±æ¯”è¾ƒç®€å•äº†ï¼ŒæŒ‰ç…§ [GitHub Copilot å¼€æºæ›¿ä»£å“ â€”â€” FauxPilot](https://alphahinex.github.io/2023/06/18/fauxpilot/) ä¸­æ–¹å¼å‡†å¤‡å¥½è¿è¡Œç¯å¢ƒï¼Œä¿®æ”¹ `.env` æ–‡ä»¶ä¸­çš„ `MODEL_DIR` ä¸ºæ–°æ¨¡å‹è·¯å¾„å³å¯ï¼Œå¦‚ `/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu`ã€‚å¦‚æœ¬æ–‡ä¸­çš„ç¤ºä¾‹å¯ä½¿ç”¨çš„ `.env` æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```.env
NUM_GPUS=2
GPUS=0,1
API_EXTERNAL_PORT=5000
TRITON_HOST=triton
TRITON_PORT=8001
MODEL=codegen-350M-multi
MODEL_DIR=/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu
HF_CACHE_DIR=/path/to/fauxpilot/.hf_cache
```

# é™„å½•

æœ¬æ–‡ä¸­æ‰€ä½¿ç”¨çš„ä¿®æ”¹åçš„è„šæœ¬ï¼ŒåŠ All in one è½¬æ¢è„šæœ¬ï¼Œå¯åœ¨ https://github.com/AlphaHinex/fauxpilot ä¸­è·å–ã€‚

# å‚è€ƒèµ„æ–™

- [How to optimize CodeGen for my code before launching FauxPilot](https://github.com/fauxpilot/fauxpilot/issues/62)
- [Guide on how to train new models on an existing codebase?](https://github.com/fauxpilot/fauxpilot/discussions/74)
- [How to convert the SalesForce CodeGen models to GPT-J](https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566)
- [Convert a SalesForce CodeGen modelâ€™s weights to plain GPT-J](https://gist.github.com/moyix/0f37da9c21c4ddfa0ab39ddad1639db4)
- [å¤§æ¨¡å‹çš„å¥½ä¼™ä¼´ï¼Œæµ…ææ¨ç†åŠ é€Ÿå¼•æ“FasterTransformer](https://zhuanlan.zhihu.com/p/626008090)