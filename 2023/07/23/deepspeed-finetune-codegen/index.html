
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç  - Alpha Hinex&#39;s Blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Java, JavaScript, Spring, Html5, NoSQL, Docker, DevOps,GitHub Copilot, Copilot, FauxPilot, VS Code, CodeGen, Triton Inference Server, GPU, FasterTransformer"> 
    <meta name="description" content="åœ¨ ç”¨ PaddleNLP ç»“åˆ CodeGen å®ç°ç¦»çº¿ GitHub Copilot å’Œ GitHub Copilot å¼€æºæ›¿ä»£å“ â€”â€” FauxPilot ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ PaddleNLP,"> 
    <meta name="author" content="Alpha Hinex"> 

    <meta name="msvalidate.01" content="D769824B4D44C14A4C777A6EC4E898FC"> 
    <meta name="baidu-site-verification" content="TimiEK4Y9V"> 
    <meta name="google-site-verification" content="B-WME81HWSnMIkZZxcxv7bVI6yjbpAFKvifi2X-EkzQ"> 

    <link rel="alternative" href="atom.xml" title="Alpha Hinex&#39;s Blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link href="https://fonts.loli.net/css?family=Roboto+Mono|Rubik&display=swap" rel="stylesheet">
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/animate.css/3.7.2/animate.min.css">

    
<link rel="stylesheet" href="/css/share.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/theme/dracula.css">

    
<link rel="stylesheet" href="/css/obsidian.css">

    
<link rel="stylesheet" href="/css/ball-atom.min.css">

<meta name="generator" content="Hexo 4.2.1"></head>


<body class="loading">
    <div class="loader">
        <div class="la-ball-atom la-2x">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
    <span id="config-title" style="display:none">Alpha Hinex&#39;s Blog</span>
    <div id="loader"></div>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation animated fadeIn fast delay-1s">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="https://AlphaHinex.github.io">
        <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div>
        <h3 class="subtitle">è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç </h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                        <a href="javascript:;" class="iconfont icon-share1"></a>
                        <div class="share-component-cc" data-disabled="facebook,douban,linkedin,diandian,tencent,google"></div>
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class=article-header-wrapper>
    <div class="article-header">
        <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/contents/fauxpilot/cover.png);">
        </div>
        <div class="else">
            <p class="animated fadeInDown">
                
                <a href="/categories/AI"><b>ã€Œ
                    </b>AI<b> ã€</b></a>
                
                ä¸ƒæœˆ 23, 2023
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/2023/07/23/deepspeed-finetune-codegen/" title="è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç " class="">è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç </a>
            </h3>
            
            <p class="post-count animated fadeInDown">
                
                <span>
                    <b class="iconfont icon-text2"></b> <i>æ–‡ç« å­—æ•°</i>
                    25k
                </span>
                
                
                <span>
                    <b class="iconfont icon-timer__s"></b> <i>é˜…è¯»çº¦éœ€</i>
                    23 mins.
                </span>
                
                
                
                <span id="busuanzi_page_pv_container" style="display: flex;">
                    <b class="iconfont icon-read"></b> <i>é˜…è¯»æ¬¡æ•°</i>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                
            </p>
            
            
            <ul class="animated fadeInDown post-tags-list" itemprop="keywords"><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/VS-Code/" rel="tag">VS Code</a></li></ul>
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn">
            <p>åœ¨ <a href="https://alphahinex.github.io/2023/06/11/paddlenlp-codegen-copilot/">ç”¨ PaddleNLP ç»“åˆ CodeGen å®ç°ç¦»çº¿ GitHub Copilot</a> å’Œ <a href="https://alphahinex.github.io/2023/06/18/fauxpilot/">GitHub Copilot å¼€æºæ›¿ä»£å“ â€”â€” FauxPilot</a> ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ PaddleNLP å’Œ FauxPilot å°† CodeGen æ¨¡å‹ä»£ç†ä¸ºå¯é€šè¿‡ HTTP è¯·æ±‚è®¿é—®çš„æ¥å£ï¼Œå¹¶é€šè¿‡ VS Code æ’ä»¶åœ¨ IDE ä¸­è·å¾—ä¸ GitHub Copilot ç±»ä¼¼çš„ AI è¾…åŠ©ç¼–ç èƒ½åŠ›ã€‚</p>
<p>ä½†ä¸è®ºæ˜¯è¿™ç§æ–¹å¼ä¹Ÿå¥½ï¼Œæˆ–è€…æ˜¯ GitHub Copilotï¼Œèƒ½å¤Ÿè¾…åŠ©ç¼–å†™çš„éƒ½æ˜¯é€šç”¨ä»£ç ï¼Œæ— æ³•è¾…åŠ©ç¼–å†™å†…éƒ¨æ¡†æ¶æˆ–ç§æœ‰ç±»åº“çš„ç›¸å…³ä»£ç ã€‚</p>
<p>è¿™ä¸ªåœºæ™¯å¯ä»¥é€šè¿‡å¯¹ CodeGen æ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥å®ç°ã€‚</p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäº <a href="https://huggingface.co/Salesforce/codegen-350M-multi" target="_blank" rel="noopener">CodeGen-350M-multi</a> æ¨¡å‹ï¼Œä½¿ç”¨ <a href="https://www.deepspeed.ai/" target="_blank" rel="noopener">DeepSpeed</a> å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨ <a href="https://github.com/fauxpilot/fauxpilot" target="_blank" rel="noopener">FauxPilot</a> é¡¹ç›®ä¸­æä¾›çš„è„šæœ¬ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè½¬æ¢ï¼Œä»¥ä½¿ç”¨ <a href="https://github.com/NVIDIA/FasterTransformer" target="_blank" rel="noopener">FasterTransformer</a> è¿›è¡ŒåŠ é€Ÿï¼Œæœ€ç»ˆåœ¨ VS Code çš„ <a href="https://github.com/Venthe/vscode-fauxpilot" target="_blank" rel="noopener">FauxPilot</a> æ’ä»¶ä¸­ï¼Œå®ç°è®© AI è¾…åŠ©ç¼–å†™å†…éƒ¨ä»£ç çš„æ•ˆæœã€‚</p>
<h1 id="æ¨¡å‹å¾®è°ƒ"><a href="#æ¨¡å‹å¾®è°ƒ" class="headerlink" title="æ¨¡å‹å¾®è°ƒ"></a>æ¨¡å‹å¾®è°ƒ</h1><h2 id="DeepSpeed-å¾®è°ƒç¯å¢ƒ"><a href="#DeepSpeed-å¾®è°ƒç¯å¢ƒ" class="headerlink" title="DeepSpeed å¾®è°ƒç¯å¢ƒ"></a>DeepSpeed å¾®è°ƒç¯å¢ƒ</h2><p>DeepSpeed ä¾èµ– <a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch</a>ï¼Œå®Œæ•´çš„ç¯å¢ƒéœ€æ±‚å¯è§å®˜æ–¹æ–‡æ¡£ <a href="https://github.com/microsoft/DeepSpeed#requirements" target="_blank" rel="noopener">Requirements</a>ï¼Œæœ¬æ–‡åœ¨ Docker é•œåƒä¸­æ‰§è¡Œå¾®è°ƒï¼Œä½¿ç”¨ <a href="https://hub.docker.com/layers/deepspeed/deepspeed/latest_torch111/images/sha256-7e594486a330c7c53be12fdc3c1b426f853a3dd1dc43d9ea1dcdf5cbc19150c4?context=explore" target="_blank" rel="noopener">deepspeed/deepspeed:latest_torch111</a> ä½œä¸ºåŸºç¡€é•œåƒï¼Œ<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">ğŸ¤— Transformers</a> <code>v4.21.1</code> ç‰ˆæœ¬ä¸­çš„ <a href="https://github.com/huggingface/transformers/blob/v4.21.1/examples/pytorch/language-modeling/run_clm.py" target="_blank" rel="noopener">run_clm.py</a> è„šæœ¬ä½œä¸ºå¾®è°ƒè„šæœ¬ï¼Œéœ€åœ¨å¾®è°ƒç¯å¢ƒä¸­å®‰è£…å¾®è°ƒè„šæœ¬æ‰€éœ€ä¾èµ– <a href="https://github.com/huggingface/transformers/blob/v4.21.1/examples/pytorch/language-modeling/requirements.txt" target="_blank" rel="noopener">requirements.txt</a> åŠ <code>aiohttp</code> å’Œ <code>transformers</code>ã€‚</p>
<p>è¿™é‡Œéœ€æ³¨æ„ <code>run_clm.py</code> å’Œ <code>requirements.txt</code> è¦ä½¿ç”¨ä¸å®‰è£…çš„ Transformers ç‰ˆæœ¬ä¸€è‡´çš„æºç  tag ä¸­çš„æ–‡ä»¶ï¼Œå¦‚ä¸Šé¢é“¾æ¥å‡ä¸º <code>v4.21.1</code> ç‰ˆæœ¬çš„ã€‚</p>
<p>å¯å‚ç…§å¦‚ä¸‹ <code>Dockerfile</code> æ„å»ºå¾®è°ƒç¯å¢ƒæ‰€ä½¿ç”¨çš„é•œåƒï¼š</p>
<pre><code class="Dockerfile">FROM deepspeed/deepspeed:latest_torch111

COPY requirements.txt requirements.txt

RUN pip install -r requirements.txt
RUN pip install aiohttp==3.6
RUN pip install transformers==4.21.1</code></pre>
<p>æ„å»ºé•œåƒï¼š</p>
<pre><code class="bash">docker build -t deepspeed:codegen .</code></pre>
<p>ä½¿ç”¨é•œåƒå¯åŠ¨å¹¶è¿›å…¥å®¹å™¨ï¼š</p>
<pre><code class="bash">$ docker run --name dstest --runtime=nvidia -v $PWD:/mnt -it deepspeed:codegen /bin/bash

=============
== PyTorch ==
=============

NVIDIA Release 21.12 (build 29870972)
PyTorch Version 1.11.0a0+b6df043

Container image Copyright (c) 2021, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.

Copyright (c) 2014-2021 Facebook Inc.
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)
Copyright (c) 2015      Google Inc.
Copyright (c) 2015      Yangqing Jia
Copyright (c) 2013-2016 The Caffe contributors
All rights reserved.

NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA CORPORATION &amp; AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION &amp; AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: MOFED driver for multi-node communication was not detected.
      Multi-node communication performance may be reduced.

NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be
   insufficient for PyTorch.  NVIDIA recommends the use of the following flags:
   docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 ...
root@f8338550c41f:/workspace#</code></pre>
<p>å®¹å™¨ä¸­ä½¿ç”¨ <code>ds_report</code> éªŒè¯ DeepSpeed çŠ¶æ€ï¼š</p>
<pre><code class="bash">root@f8338550c41f:/workspace# ds_report
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [OKAY]
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [NO] ....... [OKAY]
cpu_adagrad ............ [NO] ....... [OKAY]
fused_adam ............. [NO] ....... [OKAY]
fused_lamb ............. [NO] ....... [OKAY]
 [WARNING]  please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [NO] ....... [NO]
transformer ............ [NO] ....... [OKAY]
stochastic_transformer . [NO] ....... [OKAY]
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [NO] ....... [NO]
utils .................. [NO] ....... [OKAY]
quantizer .............. [NO] ....... [OKAY]
transformer_inference .. [NO] ....... [OKAY]
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... [&#39;/opt/conda/lib/python3.8/site-packages/torch&#39;]
torch version .................... 1.11.0a0+b6df043
torch cuda version ............... 11.5
torch hip version ................ None
nvcc version ..................... 11.5
deepspeed install path ........... [&#39;/opt/conda/lib/python3.8/site-packages/deepspeed&#39;]
deepspeed info ................... 0.6.5, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.11, cuda 11.5</code></pre>
<p>å®¹å™¨ä¸­éªŒè¯ CUDA çŠ¶æ€ï¼š</p>
<pre><code class="bash">root@f8338550c41f:/workspace# python
Python 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:59:51)
[GCC 9.4.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.cuda.is_available()
True</code></pre>
<h2 id="æ•°æ®é›†"><a href="#æ•°æ®é›†" class="headerlink" title="æ•°æ®é›†"></a>æ•°æ®é›†</h2><p>å¾®è°ƒæ•°æ®é›†è‹¥ä½¿ç”¨ Hugging Face ä¸Šçš„ï¼Œå¯ç›´æ¥åœ¨å¾®è°ƒå‘½ä»¤ä¸­ä¼ å…¥æ•°æ®é›†åç§°ï¼Œå¦‚ <a href="https://huggingface.co/datasets/moyix/debian_csrc" target="_blank" rel="noopener">moyjx/debian_csrc</a>ã€‚å¦‚éœ€ä½¿ç”¨æœ¬åœ°æ•°æ®é›†ï¼Œä»…æ”¯æŒ <code>csv/json/txt</code> æ ¼å¼ï¼Œè¿™é‡Œçš„ <code>json</code> æ ¼å¼ï¼Œæ˜¯æŒ‡ <a href="https://alphahinex.github.io/2023/07/16/json-lines/">å¤„ç†å¤§æ•°æ®é›†çš„çµæ´»æ ¼å¼ â€”â€” JSON Lines</a> ä¸­æåˆ°çš„ JSON Lines æ ¼å¼ï¼Œä¾‹å¦‚ï¼š</p>
<pre><code class="jsonl">{&quot;text&quot;: &quot;content_of_source_file_1&quot;, &quot;url&quot;: &quot;path_to_source_file_1&quot;}
{&quot;text&quot;: &quot;content_of_source_file_2&quot;, &quot;url&quot;: &quot;path_to_source_file_2&quot;}
...</code></pre>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li><code>text</code> å±æ€§ä¸ºå¿…éœ€å±æ€§ï¼Œä¿å­˜è®­ç»ƒæ•°æ®ï¼Œå³æºç æ–‡ä»¶å†…å®¹ï¼Œè¯¥å±æ€§åå¯åœ¨ <code>run_clm.py</code> è„šæœ¬ä¸­ä¿®æ”¹ã€‚</li>
<li>å…¶ä»–å±æ€§å¯è‡ªæ„¿æ·»åŠ ï¼Œå¦‚ä¸Šé¢çš„ <code>url</code> å±æ€§å¯ä»¥æ ‡è¯†æ–‡ä»¶æ¥æºã€‚</li>
</ul>
<p>å¯ä½¿ç”¨ <a href="https://github.com/AlphaHinex/go-toolkit/tree/main/files2jsonl" target="_blank" rel="noopener">files2jsonl</a> å·¥å…·å°†æºç æ–‡ä»¶å¤¹è½¬æ¢ä¸ºå¯ç›´æ¥ä½¿ç”¨çš„æœ¬åœ°æ•°æ®é›†ã€‚</p>
<h2 id="å¾®è°ƒå‘½ä»¤"><a href="#å¾®è°ƒå‘½ä»¤" class="headerlink" title="å¾®è°ƒå‘½ä»¤"></a>å¾®è°ƒå‘½ä»¤</h2><pre><code class="bash">deepspeed --num_gpus 4 --num_nodes 1 \
./run_clm.py \
--model_name_or_path=./codegen-350M-multi \
--per_device_train_batch_size=1 \
--learning_rate 2e-5 \
--num_train_epochs 1 \
--output_dir=./codegen-350M-multi-finetune \
--train_file ./test_dataset.json \
--tokenizer_name ./codegen-350M-multi \
--block_size 2048 \
--gradient_accumulation_steps 32 \
--do_train \
--fp16 \
--overwrite_output_dir \
--deepspeed ./ds_config.json</code></pre>
<p><code>train_file</code> å‚æ•°æŒ‡å®šæœ¬åœ°æ–‡ä»¶ã€‚è‹¥è¦ä½¿ç”¨ Hugging Face ä¸Šæ•°æ®é›†å¾®è°ƒï¼Œå¯ä½¿ç”¨ <code>dataset_name</code> å‚æ•°æŒ‡å®šæ•°æ®é›†åç§°ã€‚</p>
<p><code>ds_config.json</code> å¯ä½¿ç”¨ <a href="https://github.com/fauxpilot/fauxpilot/issues/62#issuecomment-1304681430" target="_blank" rel="noopener">è¿™é‡Œçš„ç¤ºä¾‹</a>:</p>
<pre><code class="json">{
    &quot;zero_optimization&quot;: {
        &quot;stage&quot;: 2,
        &quot;offload_optimizer&quot;: {
            &quot;device&quot;: &quot;cpu&quot;,
            &quot;pin_memory&quot;: true
        },
        &quot;allgather_partitions&quot;: true,
        &quot;allgather_bucket_size&quot;: 2e8,
        &quot;overlap_comm&quot;: true,
        &quot;reduce_scatter&quot;: true,
        &quot;reduce_bucket_size&quot;: 2e8,
        &quot;contiguous_gradients&quot;: true
    },
    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,
    &quot;gradient_clipping&quot;: &quot;auto&quot;,
    &quot;steps_per_print&quot;: 2000,
    &quot;train_batch_size&quot;: &quot;auto&quot;,
    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,
    &quot;wall_clock_breakdown&quot;: false
}</code></pre>
<h3 id="ä½¿ç”¨å¤šå¡å¾®è°ƒ"><a href="#ä½¿ç”¨å¤šå¡å¾®è°ƒ" class="headerlink" title="ä½¿ç”¨å¤šå¡å¾®è°ƒ"></a>ä½¿ç”¨å¤šå¡å¾®è°ƒ</h3><p><code>num_gpus</code> å‚æ•°å¯ä»¥æŒ‡å®šä½¿ç”¨çš„ GPU æ•°é‡ã€‚</p>
<p>å¦‚ä½¿ç”¨å¤šä¸ª GPU æ—¶é‡ä»¥ä¸‹æŠ¥é”™ï¼š</p>
<pre><code class="text">Pytorch &quot;NCCL error&quot;: unhandled system error, NCCL version 2.4.8&quot;</code></pre>
<p>å¯å‚ç…§ <a href="https://stackoverflow.com/questions/61075390/pytorch-nccl-error-unhandled-system-error-nccl-version-2-4-8#" target="_blank" rel="noopener">è¿™é‡Œ</a> åœ¨ <code>run_clm.py</code> ä¸­åŠ å…¥ <code>INFO</code> çº§åˆ«è°ƒè¯•ä¿¡æ¯ï¼Œå¦‚ï¼š</p>
<pre><code class="diff"> from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version

+os.environ[&quot;NCCL_DEBUG&quot;] = &quot;INFO&quot;

 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version(&quot;4.21.0&quot;)</code></pre>
<p>æŸ¥çœ‹è¯¦ç»†æŠ¥é”™ä¿¡æ¯ã€‚</p>
<p>å¦‚çœ‹åˆ°å…·ä½“ <a href="https://github.com/NVIDIA/nccl/issues/290" target="_blank" rel="noopener">æŠ¥é”™</a> ä¸ºï¼š</p>
<pre><code class="text">NCCL WARN Call to posix_fallocate failed : No space left on device</code></pre>
<p>å¯å‚ç…§ PaddlePaddle çš„ <a href="https://github.com/PaddlePaddle/Paddle/pull/28484/files" target="_blank" rel="noopener">è§£å†³æ–¹å¼</a>ï¼Œåœ¨ <code>run_clm.py</code> ä¸­åŠ å…¥ï¼š</p>
<pre><code class="diff"> from transformers.utils import check_min_version, send_example_telemetry
 from transformers.utils.versions import require_version

 os.environ[&quot;NCCL_DEBUG&quot;] = &quot;INFO&quot;
+os.environ[&#39;NCCL_SHM_DISABLE&#39;] = str(1)

 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version(&quot;4.21.0&quot;)</code></pre>
<h3 id="æŒ‡å®š-GPU"><a href="#æŒ‡å®š-GPU" class="headerlink" title="æŒ‡å®š GPU"></a>æŒ‡å®š GPU</h3><p>è¦æŒ‡å®š GPU æ—¶ï¼Œå¯å‚ç…§ <a href="https://zhuanlan.zhihu.com/p/624223085" target="_blank" rel="noopener">deepspeedå¤šæœºå¤šå¡è®­ç»ƒè¸è¿‡çš„å‘</a> ä¸­å†…å®¹ï¼Œå»æ‰ <code>num_gpus</code> å’Œ <code>num_nodes</code> å‚æ•°ï¼Œä½¿ç”¨ <code>--include localhost:1,2</code> å½¢å¼é…ç½®å•æœºå¤šå¡ã€‚</p>
<h2 id="å¾®è°ƒè€—æ—¶è¯„ä¼°"><a href="#å¾®è°ƒè€—æ—¶è¯„ä¼°" class="headerlink" title="å¾®è°ƒè€—æ—¶è¯„ä¼°"></a>å¾®è°ƒè€—æ—¶è¯„ä¼°</h2><p>ä½¿ç”¨ä¸€ä¸ª Tesla P40ï¼ˆ24G VRAMï¼‰å¾®è°ƒ CodeGen-350M-multi æ¨¡å‹ï¼Œæ˜¾å­˜ä½¿ç”¨ 23G å·¦å³ï¼Œå¾®è°ƒæ—¶é—´ï¼š</p>
<ol>
<li>40w è¡Œé‚®ç®±æ•°æ®ï¼Œ24M è®­ç»ƒæ•°æ®é›†ï¼Œå¤§çº¦è€—æ—¶ 10 åˆ†é’Ÿ</li>
<li>300 ä¸ª java æ–‡ä»¶ï¼Œ75M è®­ç»ƒæ•°æ®é›†ï¼Œå¤§çº¦è€—æ—¶ 1 å°æ—¶ 20 åˆ†é’Ÿ</li>
</ol>
<p>åœ¨ <a href="https://github.com/fauxpilot/fauxpilot/discussions/74#discussioncomment-3798458" target="_blank" rel="noopener">https://github.com/fauxpilot/fauxpilot/discussions/74#discussioncomment-3798458</a> ä¸­ï¼ŒFauxPilot ä½œè€…ä¹Ÿç»™å‡ºäº†ä»–ä»¬å¾®è°ƒ 16B æ¨¡å‹çš„èµ„æºéœ€æ±‚æƒ…å†µï¼š</p>
<blockquote>
<p>As a warning, fine-tuning or training large models (like CodeGen 16B) takes a lot of GPU resources â€“ we fine-tuned a 16B model on Verilog code, and it took 3xA100 GPUs with 80GB of VRAM each running for six days to do one pass over the 400MB dataset.</p>
</blockquote>
<h2 id="å¾®è°ƒåéªŒè¯"><a href="#å¾®è°ƒåéªŒè¯" class="headerlink" title="å¾®è°ƒåéªŒè¯"></a>å¾®è°ƒåéªŒè¯</h2><p>æ¨¡å‹å¾®è°ƒä¹‹åï¼Œå¯é€šè¿‡å¦‚ä¸‹ Python ä»£ç è¿›è¡ŒéªŒè¯ï¼š</p>
<pre><code class="python">from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(&quot;/path/to/codegen-350M-multi-finetune&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;/path/to/codegen-350M-multi-finetune&quot;)

text = &quot;def qucik_sort&quot;
input_ids = tokenizer(text, return_tensors=&quot;pt&quot;).input_ids

generated_ids = model.generate(input_ids, max_length=128)
print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))</code></pre>
<p>åœ¨ <code>text</code> ä¸­æ”¾å…¥æç¤ºè¯ï¼Œè§‚å¯Ÿ <code>print</code> è¾“å‡ºçš„ç»“æœï¼Œæ˜¯å¦å­¦ä¹ åˆ°äº†è®­ç»ƒæ•°æ®ä¸­çš„å†…å®¹ã€‚</p>
<h1 id="æ¨¡å‹è½¬æ¢"><a href="#æ¨¡å‹è½¬æ¢" class="headerlink" title="æ¨¡å‹è½¬æ¢"></a>æ¨¡å‹è½¬æ¢</h1><p>åœ¨é€šè¿‡ä¸Šé¢çš„ Python ä»£ç éªŒè¯å¾®è°ƒåçš„æ¨¡å‹èƒ½åŠ›æ—¶ï¼Œå¯ä»¥æ„Ÿå—åˆ°éœ€è¦çš„æ—¶é—´è¿˜æ˜¯å¾ˆé•¿çš„ï¼Œè¿™ä¸ªæ—¶é—´é•¿åˆ°æ— æ³•æ»¡è¶³åœ¨ IDE ä¸­å³æ—¶è¡¥å…¨ä»£ç çš„éœ€æ±‚ã€‚</p>
<p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒFauxPilot çš„ä½œè€…ä½¿ç”¨äº† <a href="https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566" target="_blank" rel="noopener">çº¿æ€§ä»£æ•°çš„æ–¹æ³•</a>ï¼Œé€šè¿‡ <a href="https://gist.github.com/moyix/0f37da9c21c4ddfa0ab39ddad1639db4" target="_blank" rel="noopener">gist ä¸Šçš„ codegen_gptj_convert.py</a> æˆ– <a href="https://github.com/fauxpilot/fauxpilot/blob/main/converter/codegen_gptj_convert.py" target="_blank" rel="noopener">ä»“åº“ä¸­çš„ codegen_gptj_convert.py</a> è½¬æ¢è„šæœ¬ï¼Œå°† CodeGen æ¨¡å‹è½¬æ¢ä¸ºäº† <a href="https://github.com/kingoflolz/mesh-transformer-jax#gpt-j-6b" target="_blank" rel="noopener">GPT-J</a> æ¨¡å‹ã€‚</p>
<p>ä¹‹æ‰€ä»¥è½¬æ¢æˆ GPT-J æ¨¡å‹ï¼Œæ˜¯å› ä¸ºè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ¶æ„ä¸Šæœ‰ 99.9% çš„ç›¸ä¼¼ï¼Œå¹¶ä¸” GPT-J åœ¨æ¨ç†åŠ é€Ÿå¼•æ“ <a href="https://github.com/NVIDIA/FasterTransformer/" target="_blank" rel="noopener">FasterTransformer</a> çš„ <a href="https://github.com/NVIDIA/FasterTransformer/#support-matrix" target="_blank" rel="noopener">æ”¯æŒåˆ—è¡¨</a> ä¸­ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬ä¼šå‘ç°åœ¨ä½¿ç”¨ FauxPilot æ—¶ï¼Œæ˜¯å»ä½œè€…è‡ªå·±çš„ Hugging Face æ¨¡å‹ä»“åº“ä¸­ä¸‹è½½è½¬æ¢åçš„æ¨¡å‹ï¼ˆå¦‚ <a href="https://huggingface.co/moyix/codegen-350M-multi-gptj" target="_blank" rel="noopener">https://huggingface.co/moyix/codegen-350M-multi-gptj</a> ï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨ Salesforce å‘å¸ƒçš„åŸå§‹æ¨¡å‹çš„åŸå› ã€‚</p>
<p>åŸå§‹çš„ CodeGen æ¨¡å‹éœ€è¦ 12 ç§’ç”Ÿæˆ 128 ä¸ª tokenï¼Œç»è¿‡æ¨ç†åŠ é€Ÿåï¼Œåœ¨ä¸€ä¸ª A6000 GPU ä¸Šå¯ä»¥å°†è€—æ—¶ç¼©çŸ­åˆ° 5.7 ç§’ï¼Œå¹¶ä¸”ä½¿ç”¨å¤š GPU è¿˜æœ‰è¿›ä¸€æ­¥åŠ é€Ÿçš„å¯èƒ½ã€‚</p>
<p>å¯é€šè¿‡å¦‚ä¸‹æ­¥éª¤ï¼Œå°†æˆ‘ä»¬å¾®è°ƒå¥½çš„ CodeGen æ¨¡å‹ï¼Œè½¬æ¢ä¸ºå¯åœ¨ FauxPilot Server ä¸­ä½¿ç”¨çš„å½¢å¼ã€‚</p>
<h2 id="codegen-gptj-convert-py"><a href="#codegen-gptj-convert-py" class="headerlink" title="codegen_gptj_convert.py"></a>codegen_gptj_convert.py</h2><p>å…ˆä½¿ç”¨ <a href="https://github.com/fauxpilot/fauxpilot/blob/main/converter/codegen_gptj_convert.py" target="_blank" rel="noopener">codegen_gptj_convert.py</a> è„šæœ¬ï¼Œå°† Salesforce CodeGen æ¨¡å‹è½¬æ¢ä¸º GPT-J æ¨¡å‹ã€‚</p>
<p>è½¬æ¢æœ¬åœ°å¾®è°ƒåçš„æ¨¡å‹æ—¶ï¼Œéœ€ä¿®æ”¹è„šæœ¬å†…å®¹ï¼Œå»æ‰ <code>choices=CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST, default=&#39;Salesforce/codegen-350M-multi&#39;,</code> è¡Œï¼š</p>
<pre><code class="diff"> parser.add_argument(&#39;--code_model&#39;,
-                    choices=CODEGEN_PRETRAINED_MODEL_ARCHIVE_LIST, default=&#39;Salesforce/codegen-350M-multi&#39;,
                     help=&#39;which SalesForce model to convert&#39;
                     )</code></pre>
<p>ä½¿ç”¨ä¸‹é¢å‘½ä»¤æ‰§è¡Œè½¬æ¢ï¼š</p>
<pre><code class="bash">python /path/to/codegen_gptj_convert.py \
--code_model /path/to/codegen-350M-multi-finetune \
/path/to/codegen-350M-multi-finetune-gptj</code></pre>
<p>è½¬æ¢æ—¶éœ€è¦ <code>code_model</code> è·¯å¾„å†…çš„ <code>pytorch_model.bin</code> å’Œ <code>config.json</code> æ–‡ä»¶ï¼Œè½¬æ¢åæ¨¡å‹ä»ä¸ºä¸€ä¸ª <code>pytorch_model.bin</code> æ–‡ä»¶ï¼Œä½†å†…å®¹å‘ç”Ÿäº†å˜åŒ–ï¼Œé…å¥—çš„ <code>config.json</code> æ–‡ä»¶ä¹Ÿä¸ä¸€æ ·äº†ã€‚</p>
<blockquote>
<p>è„šæœ¬ç”¨æ³•å¯å‚ç…§ <a href="https://github.com/fauxpilot/fauxpilot/blob/main/converter/download_and_convert_model.sh" target="_blank" rel="noopener">download_and_convert_model.sh</a>ã€‚</p>
</blockquote>
<h2 id="triton-config-gen-py"><a href="#triton-config-gen-py" class="headerlink" title="triton_config_gen.py"></a>triton_config_gen.py</h2><p>è½¬æ¢åçš„ GPT-J æ¨¡å‹åœ¨ç»è¿‡ FasterTransformer åŠ é€Ÿåï¼Œæœ€ç»ˆä¼šéƒ¨ç½²åˆ° <a href="https://github.com/triton-inference-server/backend" target="_blank" rel="noopener">Triton Inference Server</a> ä¸­ã€‚éœ€å…ˆä½¿ç”¨ <a href="https://github.com/fauxpilot/fauxpilot/blob/main/converter/triton_config_gen.py" target="_blank" rel="noopener">triton_config_gen.py</a> è„šæœ¬æ¥ç”Ÿæˆ Triton éœ€ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ã€‚</p>
<p>ä½†åœ¨ä½¿ç”¨ FauxPilot ä»“åº“ä¸­çš„è¿™ä¸ªè„šæœ¬ç”Ÿæˆ CodeGen-350M-multi å¾®è°ƒåæ¨¡å‹çš„é…ç½®æ—¶ï¼Œ<code>vocab_size</code> çš„ç®—æ³•éœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œå¦åˆ™ä½¿ç”¨è½¬æ¢åçš„æ¨¡å‹æ—¶ä¼šå‡ºç°è¡¥å…¨çš„éƒ½æ˜¯æ··ä¹±å†…å®¹çš„æƒ…å†µï¼š</p>
<pre><code class="diff"> # Vocab size *sometimes* gets rounded up to a multiple of 1024
-params[&#39;vocab_size&#39;] = tokenizer.vocab_size+len(tokenizer.get_added_vocab())  # round_up(tokenizer.vocab_size, 1024)
+params[&#39;vocab_size&#39;] = round_up(tokenizer.vocab_size, 1024)
 params[&#39;start_id&#39;] = tokenizer.eos_token_id</code></pre>
<p>è°ƒæ•´è„šæœ¬åæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ç”Ÿæˆé…ç½®ï¼š</p>
<pre><code class="bash">python /path/to/triton_config_gen.py -n 2 \
--tokenizer /path/to/codegen-350M-multi-finetune \
--hf_model_dir /path/to/codegen-350M-multi-finetune-gptj \
--model_store /path/to/fauxpilot/models \
--rebase /model</code></pre>
<blockquote>
<p><code>triton_config_gen.py</code> è„šæœ¬éœ€ä¸ <a href="https://github.com/fauxpilot/fauxpilot/blob/main/converter/config_template.pbtxt" target="_blank" rel="noopener">config_template.pbtxt</a> æ¨¡æ¿æ–‡ä»¶æ”¾åœ¨ç›¸åŒè·¯å¾„ä¸‹å…±åŒä½¿ç”¨ã€‚</p>
</blockquote>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li><code>-n</code> ä¸ºæœ€ç»ˆè¿è¡Œæ—¶éœ€è¦ä½¿ç”¨çš„ GPU æ•°é‡</li>
<li><code>--tokenizer</code> æŒ‡å®šå¾®è°ƒåçš„ CodeGen æ¨¡å‹è·¯å¾„ï¼ˆå› ä¸ºä½¿ç”¨ <code>codegen_gptj_convert.py</code> è„šæœ¬è½¬æ¢å¾—åˆ°çš„ GPT-J æ¨¡å‹è·¯å¾„ä¸­åªæœ‰ <code>pytorch_model.bin</code> å’Œ <code>config.json</code> ä¸¤ä¸ªæ–‡ä»¶ï¼‰</li>
<li><code>--hf_model_dir</code> æŒ‡å®šè½¬æ¢åçš„ GPT-J æ¨¡å‹è·¯å¾„</li>
<li><code>--model_store</code> æŒ‡å®šé…ç½®æ–‡ä»¶çš„ç”Ÿæˆè·¯å¾„</li>
<li><code>--rebase</code> ç”¨æ¥æŒ‡å®šå°† FasterTransformer åŠ é€Ÿåçš„æ¨¡å‹æ–‡ä»¶æŒ‚è½½åˆ°å®¹å™¨é‡Œæ—¶ï¼Œå®¹å™¨å†…æ‰€ä½¿ç”¨çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚å¦‚ä½¿ç”¨ FauxPilot æä¾›çš„ Docker Compose æ–¹å¼å¯åŠ¨ FauxPilot Server æœåŠ¡ï¼Œå¯ä¿æŒä½¿ç”¨ <code>/model</code> è·¯å¾„ä¸å˜</li>
</ul>
<p>ä»¥ä¸Šé¢çš„å‘½ä»¤ä¸ºä¾‹ï¼Œæ‰§è¡ŒæˆåŠŸåä¼šåœ¨ <code>/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer</code> è·¯å¾„ä¸‹ç”Ÿæˆä¸€ä¸ª <code>config.pbtxt</code> æ–‡ä»¶ã€‚</p>
<h2 id="huggingface-gptj-convert-py"><a href="#huggingface-gptj-convert-py" class="headerlink" title="huggingface_gptj_convert.py"></a>huggingface_gptj_convert.py</h2><p>ä½¿ç”¨ <a href="https://github.com/fauxpilot/fauxpilot/blob/main/converter/huggingface_gptj_convert.py" target="_blank" rel="noopener">huggingface_gptj_convert.py</a> è„šæœ¬å°† GPT-J æ¨¡å‹è½¬æ¢æˆ FasterTransformer æ ¼å¼ï¼š</p>
<pre><code class="bash">python /path/to/huggingface_gptj_convert.py \
-in_file /path/to/codegen-350M-multi-finetune-gptj \
-saved_dir /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1 \
-infer_gpu_num 2</code></pre>
<p>å…¶ä¸­ï¼š</p>
<ul>
<li><code>in_file</code> ä¸ºè½¬æ¢æˆ GPT-J æ ¼å¼çš„å¾®è°ƒåæ¨¡å‹æ–‡ä»¶è·¯å¾„</li>
<li><code>saved_dir</code> ä¸ºä¸Šé¢ <code>triton_config_gen.py</code> è„šæœ¬ç”Ÿæˆé…ç½®æ–‡ä»¶çš„è·¯å¾„åŠ ä¸€å±‚ <code>/1</code></li>
<li><code>infer_gpu_num</code> ä¸ºæ¨ç†æ‰€ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œæ³¨æ„éœ€ä¸ <code>triton_config_gen.py</code> è„šæœ¬çš„ <code>-n</code> å‚æ•°å€¼ä¸€è‡´</li>
</ul>
<h2 id="All-in-one-è„šæœ¬"><a href="#All-in-one-è„šæœ¬" class="headerlink" title="All in one è„šæœ¬"></a>All in one è„šæœ¬</h2><p>å¯ä½¿ç”¨ <a href="https://github.com/AlphaHinex/fauxpilot/blob/350m/converter/convert_model.sh" target="_blank" rel="noopener">convert_model.sh</a> è„šæœ¬å®Œæˆä¸Šè¿°æ‰€æœ‰è½¬æ¢å·¥ä½œï¼Œç”¨æ³•ä¸ºï¼š</p>
<pre><code class="bash">./convert_model.sh codegen-350M-multi-finetune 2</code></pre>
<p>å°†å¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„æ”¾è‡³è¯¥è„šæœ¬è·¯å¾„å†…ï¼Œå¹¶å°†è¯¥è„šæœ¬ä¸å…¶ä»–è½¬æ¢æ‰€éœ€è„šæœ¬å’Œæ¨¡æ¿æ–‡ä»¶æ”¾ç½®åœ¨ç›¸åŒè·¯å¾„ä¸‹ã€‚ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºå¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œç¬¬äºŒä¸ªå‚æ•°ä¸ºæ¨ç†æ—¶éœ€ä½¿ç”¨çš„ GPU æ•°é‡ã€‚</p>
<p>è¯¥è„šæœ¬å†…å®¹å¦‚ä¸‹ï¼š</p>
<pre><code class="bash">#!/bin/bash

MODEL=${1}
NUM_GPUS=${2}

echo &quot;Converting model ${MODEL} with ${NUM_GPUS} GPUs&quot;

python3 codegen_gptj_convert.py --code_model ./${MODEL} ${MODEL}-gptj

rm -rf ./models/${MODEL}-${NUM_GPUS}gpu

python3 triton_config_gen.py -n ${NUM_GPUS} --tokenizer ./${MODEL} --hf_model_dir ${MODEL}-gptj --model_store ./models --rebase /model

python3 huggingface_gptj_convert.py -in_file ${MODEL}-gptj -saved_dir ./models/${MODEL}-gptj-${NUM_GPUS}gpu/fastertransformer/1 -infer_gpu_num ${NUM_GPUS}

rm -rf ${MODEL}-gptj</code></pre>
<p>æ‰§è¡ŒæˆåŠŸåï¼Œä¼šåœ¨è„šæœ¬æ‰€åœ¨ä½ç½®çš„ <code>models/codegen-350M-multi-finetune-gptj-2gpu</code> ä¸‹è·å¾—è½¬æ¢å¥½çš„æ¨¡å‹æ–‡ä»¶ã€‚</p>
<h2 id="æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶"><a href="#æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶" class="headerlink" title="æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶"></a>æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶</h2><p>å®é™…ä½¿ç”¨æ—¶å‘ç°ï¼Œç»è¿‡ä¸Šè¿°è¿‡ç¨‹è½¬æ¢åçš„æ¨¡å‹åœ¨ FauxPilot Server ä¸­ä½¿ç”¨æ—¶ï¼Œä¼šå‡ºç°è¡¥å…¨çš„ä»£ç å†…å®¹éƒ½æ˜¯æ··ä¹±çš„æ— æ³•è¾¨è¯†å†…å®¹ï¼Œç»è¯•éªŒå‘ç°éœ€è¦ä½¿ç”¨ FauxPilot ä½¿ç”¨çš„åŸå§‹æ¨¡å‹ä¸­çš„éƒ¨åˆ†æ–‡ä»¶æ›¿æ¢é€šè¿‡ä¸Šè¿°æ–¹å¼è½¬æ¢ä¹‹åçš„ FasterTransformer æ¨¡å‹æ–‡ä»¶ã€‚ä»¥ <code>CodeGen-350M-multi</code> ä¸ºä¾‹ï¼Œéœ€æ›¿æ¢çš„æ–‡ä»¶ä¸ºï¼š</p>
<pre><code class="text">model.lm_head.bias.bin
model.lm_head.weight.bin
model.wte.bin</code></pre>
<p>å¯åœ¨ <a href="https://huggingface.co/moyix/codegen-350M-multi-gptj/tree/main" target="_blank" rel="noopener">https://huggingface.co/moyix/codegen-350M-multi-gptj/tree/main</a> å¯¹åº” GPU æ•°é‡çš„ <code>zst</code> å‹ç¼©æ–‡ä»¶ä¸­ï¼Œæå–ä¸Šè¿°æ–‡ä»¶ï¼ˆæˆ–ä½¿ç”¨ Salesforce åŸå§‹æ¨¡å‹é€šè¿‡ä¸Šè¿°è¿‡ç¨‹è½¬æ¢å¾—åˆ°ï¼Œä¸å¾®è°ƒç›´æ¥è½¬æ¢æ—¶è¿™ä¸‰ä¸ªæ–‡ä»¶å†…å®¹åº”è¯¥æ˜¯æ­£ç¡®çš„ï¼Œå¯ä»¥åœ¨ FauxPilot Server ä¸­æ­£å¸¸ä½¿ç”¨ï¼‰ï¼Œå¹¶è¦†ç›–è‡ªè¡Œè½¬æ¢å‡ºçš„æ–‡ä»¶ï¼Œå¦‚ï¼š</p>
<pre><code class="bash">cp /path/to/origin/codegen-350M-multi-2gpu/fastertransformer/1/2-gpu/model.lm_head.bias.bin /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1/2-gpu/model.lm_head.bias.bin
cp /path/to/origin/codegen-350M-multi-2gpu/fastertransformer/1/2-gpu/model.lm_head.weight.bin /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1/2-gpu/model.lm_head.weight.bin
cp /path/to/origin/codegen-350M-multi-2gpu/fastertransformer/1/2-gpu/model.wte.bin /path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu/fastertransformer/1/2-gpu/model.wte.bin</code></pre>
<h2 id="æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘"><a href="#æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘" class="headerlink" title="æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘"></a>æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘</h2><pre><code class="bash">$ pwd
/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu
$ tree -L 4
.
â””â”€â”€ fastertransformer
    â”œâ”€â”€ 1
    â”‚   â””â”€â”€ 2-gpu
    â”‚       â”œâ”€â”€ config.ini
    â”‚       â”œâ”€â”€ model.final_layernorm.bias.bin
    â”‚       â”œâ”€â”€ model.final_layernorm.weight.bin
    â”‚       â”œâ”€â”€ model.layers.0.attention.dense.weight.0.bin
    â”‚       â”œâ”€â”€ model.layers.0.attention.query_key_value.weight.0.bin
    â”‚       â”œâ”€â”€ model.layers.0.input_layernorm.bias.bin
    â”‚       â”œâ”€â”€ model.layers.0.input_layernorm.weight.bin
    â”‚       â”œâ”€â”€ model.layers.0.mlp.dense_4h_to_h.bias.bin
    â”‚       â”œâ”€â”€ model.layers.0.mlp.dense_4h_to_h.weight.0.bin
    â”‚       â”œâ”€â”€ model.layers.0.mlp.dense_h_to_4h.bias.0.bin
    â”‚       â”œâ”€â”€ model.layers.0.mlp.dense_h_to_4h.weight.0.bin
...
    â”‚       â”œâ”€â”€ model.layers.9.attention.dense.weight.0.bin
    â”‚       â”œâ”€â”€ model.layers.9.attention.query_key_value.weight.0.bin
    â”‚       â”œâ”€â”€ model.layers.9.input_layernorm.bias.bin
    â”‚       â”œâ”€â”€ model.layers.9.input_layernorm.weight.bin
    â”‚       â”œâ”€â”€ model.layers.9.mlp.dense_4h_to_h.bias.bin
    â”‚       â”œâ”€â”€ model.layers.9.mlp.dense_4h_to_h.weight.0.bin
    â”‚       â”œâ”€â”€ model.layers.9.mlp.dense_h_to_4h.bias.0.bin
    â”‚       â”œâ”€â”€ model.layers.9.mlp.dense_h_to_4h.weight.0.bin
    â”‚       â”œâ”€â”€ model.lm_head.bias.bin
    â”‚       â”œâ”€â”€ model.lm_head.weight.bin
    â”‚       â””â”€â”€ model.wte.bin
    â””â”€â”€ config.pbtxt</code></pre>
<h1 id="æ–°æ¨¡å‹ä½¿ç”¨"><a href="#æ–°æ¨¡å‹ä½¿ç”¨" class="headerlink" title="æ–°æ¨¡å‹ä½¿ç”¨"></a>æ–°æ¨¡å‹ä½¿ç”¨</h1><p>åœ¨ FauxPilot ä¸­ä½¿ç”¨å¾®è°ƒå¹¶è½¬æ¢åçš„æ–°æ¨¡å‹å°±æ¯”è¾ƒç®€å•äº†ï¼ŒæŒ‰ç…§ <a href="https://alphahinex.github.io/2023/06/18/fauxpilot/">GitHub Copilot å¼€æºæ›¿ä»£å“ â€”â€” FauxPilot</a> ä¸­æ–¹å¼å‡†å¤‡å¥½è¿è¡Œç¯å¢ƒï¼Œä¿®æ”¹ <code>.env</code> æ–‡ä»¶ä¸­çš„ <code>MODEL_DIR</code> ä¸ºæ–°æ¨¡å‹è·¯å¾„å³å¯ï¼Œå¦‚ <code>/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu</code>ã€‚å¦‚æœ¬æ–‡ä¸­çš„ç¤ºä¾‹å¯ä½¿ç”¨çš„ <code>.env</code> æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š</p>
<pre><code class=".env">NUM_GPUS=2
GPUS=0,1
API_EXTERNAL_PORT=5000
TRITON_HOST=triton
TRITON_PORT=8001
MODEL=codegen-350M-multi
MODEL_DIR=/path/to/fauxpilot/models/codegen-350M-multi-finetune-gptj-2gpu
HF_CACHE_DIR=/path/to/fauxpilot/.hf_cache</code></pre>
<h1 id="é™„å½•"><a href="#é™„å½•" class="headerlink" title="é™„å½•"></a>é™„å½•</h1><p>æœ¬æ–‡ä¸­æ‰€ä½¿ç”¨çš„ä¿®æ”¹åçš„è„šæœ¬ï¼ŒåŠ All in one è½¬æ¢è„šæœ¬ï¼Œå¯åœ¨ <a href="https://github.com/AlphaHinex/fauxpilot" target="_blank" rel="noopener">https://github.com/AlphaHinex/fauxpilot</a> ä¸­è·å–ã€‚</p>
<h1 id="å‚è€ƒèµ„æ–™"><a href="#å‚è€ƒèµ„æ–™" class="headerlink" title="å‚è€ƒèµ„æ–™"></a>å‚è€ƒèµ„æ–™</h1><ul>
<li><a href="https://github.com/fauxpilot/fauxpilot/issues/62" target="_blank" rel="noopener">How to optimize CodeGen for my code before launching FauxPilot</a></li>
<li><a href="https://github.com/fauxpilot/fauxpilot/discussions/74" target="_blank" rel="noopener">Guide on how to train new models on an existing codebase?</a></li>
<li><a href="https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566" target="_blank" rel="noopener">How to convert the SalesForce CodeGen models to GPT-J</a></li>
<li><a href="https://gist.github.com/moyix/0f37da9c21c4ddfa0ab39ddad1639db4" target="_blank" rel="noopener">Convert a SalesForce CodeGen modelâ€™s weights to plain GPT-J</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/626008090" target="_blank" rel="noopener">å¤§æ¨¡å‹çš„å¥½ä¼™ä¼´ï¼Œæµ…ææ¨ç†åŠ é€Ÿå¼•æ“FasterTransformer</a></li>
</ul>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls"
                data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
            <ul id="audio-list" style="display:none">
                
                
                <li title='0' data-url='/statics/background.mp3'></li>
                
                    
            </ul>
            
            
            
    <div id='gitalk-container' class="comment link"
        data-ae='true'
        data-ci='54f9966b8cc9d2423ffd'
        data-cs='504574e4532bdfa77d3e4091637ff53558408ac2'
        data-r='AlphaHinex.github.io'
        data-o='AlphaHinex'
        data-a='AlphaHinex'
        data-d=''
    >ç•™è¨€</div>


            
            
        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <img src="/img/hinex.jpg" height=300 width=300></img>
                    <p>Alpha Hinex</p>
                    <span>Stay Hungry. Stay Foolish.</span>
                    <dl>
                        <dd><a href="https://github.com/AlphaHinex" target="_blank"><span
                                    class=" iconfont icon-github"></span></a></dd>
                        <dd><a href="/whoami"><span class=" iconfont icon-wechat"></span></a></dd>
                        <dd><a href="" target="_blank"><span
                                    class=" iconfont icon-stack-overflow"></span></a></dd>
                    </dl>
                </div>
                <ul>
                    <li><a href="/">331 <p>æ–‡ç« </p></a></li>
                    <li><a href="/categories">78 <p>åˆ†ç±»</p></a></li>
                    <li><a href="/tags">159 <p>æ ‡ç­¾</p></a></li>
                </ul>
            </div>
            
            
            
            <div class="box sticky animated fadeInRight faster">
                <div id="toc" class="subbox">
                    <h4>ç›®å½•</h4>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#æ¨¡å‹å¾®è°ƒ"><span class="toc-number">1.</span> <span class="toc-text">æ¨¡å‹å¾®è°ƒ</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DeepSpeed-å¾®è°ƒç¯å¢ƒ"><span class="toc-number">1.1.</span> <span class="toc-text">DeepSpeed å¾®è°ƒç¯å¢ƒ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#æ•°æ®é›†"><span class="toc-number">1.2.</span> <span class="toc-text">æ•°æ®é›†</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#å¾®è°ƒå‘½ä»¤"><span class="toc-number">1.3.</span> <span class="toc-text">å¾®è°ƒå‘½ä»¤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ä½¿ç”¨å¤šå¡å¾®è°ƒ"><span class="toc-number">1.3.1.</span> <span class="toc-text">ä½¿ç”¨å¤šå¡å¾®è°ƒ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#æŒ‡å®š-GPU"><span class="toc-number">1.3.2.</span> <span class="toc-text">æŒ‡å®š GPU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#å¾®è°ƒè€—æ—¶è¯„ä¼°"><span class="toc-number">1.4.</span> <span class="toc-text">å¾®è°ƒè€—æ—¶è¯„ä¼°</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#å¾®è°ƒåéªŒè¯"><span class="toc-number">1.5.</span> <span class="toc-text">å¾®è°ƒåéªŒè¯</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#æ¨¡å‹è½¬æ¢"><span class="toc-number">2.</span> <span class="toc-text">æ¨¡å‹è½¬æ¢</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#codegen-gptj-convert-py"><span class="toc-number">2.1.</span> <span class="toc-text">codegen_gptj_convert.py</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#triton-config-gen-py"><span class="toc-number">2.2.</span> <span class="toc-text">triton_config_gen.py</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#huggingface-gptj-convert-py"><span class="toc-number">2.3.</span> <span class="toc-text">huggingface_gptj_convert.py</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#All-in-one-è„šæœ¬"><span class="toc-number">2.4.</span> <span class="toc-text">All in one è„šæœ¬</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶"><span class="toc-number">2.5.</span> <span class="toc-text">æ›¿æ¢éƒ¨åˆ†æ–‡ä»¶</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘"><span class="toc-number">2.6.</span> <span class="toc-text">æ¨¡å‹è½¬æ¢è¿‡ç¨‹æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ ‘</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#æ–°æ¨¡å‹ä½¿ç”¨"><span class="toc-number">3.</span> <span class="toc-text">æ–°æ¨¡å‹ä½¿ç”¨</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#é™„å½•"><span class="toc-number">4.</span> <span class="toc-text">é™„å½•</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#å‚è€ƒèµ„æ–™"><span class="toc-number">5.</span> <span class="toc-text">å‚è€ƒèµ„æ–™</span></a></li></ol>
                </div>
            </div>
            
            
        </div>
    </div>
</div>

    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p>æœ¬ç«™è®¿å®¢æ•°<span id="busuanzi_value_site_uv"></span>äººæ¬¡ / æœ¬ç«™æ€»è®¿é—®é‡<span id="busuanzi_value_site_pv"></span>æ¬¡</p>
    <p class="copyright" id="copyright">
        &copy; 2026
        <span class="gradient-text">
            Alpha Hinex
        </span>.
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
        Theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.3" target="_blank" rel="noopener">v1.4.3</a></small>
    </p>
</footer>

<script type="text/javascript" src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX", "TeX"],
      linebreaks: {
        automatic: true
      },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"]
      ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      noUndefined: {
        attributes: {
          mathcolor: "red",
          mathbackground: "#FFEEEE",
          mathsize: "90%"
        }
      },
      Macros: {
        href: "{}"
      }
    },
    messageStyle: "none"
  });
</script>
<script>
  function initialMathJax() {
    MathJax.Hub.Queue(function () {
      var all = MathJax.Hub.getAllJax(),
        i;
      // console.log(all);
      for (i = 0; i < all.length; i += 1) {
        console.log(all[i].SourceElement().parentNode)
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  }

  function reprocessMathJax() {
    if (typeof MathJax !== 'undefined') {
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    }
  }
</script>



    
<link rel="stylesheet" href="/css/gitalk.css">

    
<script src="/js/gitalk.min.js"></script>



<script src="/js/jquery-3.4.1.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/typed.js/2.0.10/typed.min.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/blueimp-md5/2.12.0/js/md5.min.js"></script>


<script src="/js/social-share.min.js"></script>


<script src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.js"></script>

    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/javascript/javascript.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/css/css.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/xml/xml.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/clike/clike.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/php/php.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/shell/shell.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/python/python.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/groovy/groovy.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/diff/diff.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/nginx/nginx.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/properties/properties.min.js"></script>




    
<script src="/js/busuanzi.min.js"></script>

    <script>
        $(document).ready(function () {
            if ($('span[id^="busuanzi_"]').length) {
                initialBusuanzi();
            }
        });
    </script>



<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-69084811-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-69084811-1');
    </script>





<script>
    function initialTyped () {
        var typedTextEl = $('.typed-text');
        if (typedTextEl && typedTextEl.length > 0) {
            var typed = new Typed('.typed-text', {
                strings: ["Stay Hungry. Stay Foolish.", "å¸¸ä¸åŒå¥½äº‰é«˜ä¸‹ï¼Œè«ä¸å‚»å­è®ºçŸ­é•¿"],
                typeSpeed: 90,
                loop: true,
                loopCount: Infinity,
                backSpeed: 20,
            });
        }
    }

    if ($('.article-header') && $('.article-header').length) {
        $(document).ready(function () {
            initialTyped();
        });
    }
</script>




</html>
