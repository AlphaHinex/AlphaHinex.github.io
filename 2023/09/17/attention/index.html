
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>【译】可视化神经机器翻译模型（Seq2seq 模型的注意力机制） - Alpha Hinex&#39;s Blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Java, JavaScript, Spring, Html5, NoSQL, Docker, DevOps,Attention, Sequence-to-sequence, deep learning, machine translation, RNN, encoder, decoder, context, hidden state, vector, embedding, feedforward neural network"> 
    <meta name="description" content="
原文地址：https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models,"> 
    <meta name="author" content="Alpha Hinex"> 

    <meta name="msvalidate.01" content="D769824B4D44C14A4C777A6EC4E898FC"> 
    <meta name="baidu-site-verification" content="TimiEK4Y9V"> 
    <meta name="google-site-verification" content="B-WME81HWSnMIkZZxcxv7bVI6yjbpAFKvifi2X-EkzQ"> 

    <link rel="alternative" href="atom.xml" title="Alpha Hinex&#39;s Blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link href="https://fonts.loli.net/css?family=Roboto+Mono|Rubik&display=swap" rel="stylesheet">
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/animate.css/3.7.2/animate.min.css">

    
<link rel="stylesheet" href="/css/share.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/theme/dracula.css">

    
<link rel="stylesheet" href="/css/obsidian.css">

    
<link rel="stylesheet" href="/css/ball-atom.min.css">

<meta name="generator" content="Hexo 4.2.1"></head>


<body class="loading">
    <div class="loader">
        <div class="la-ball-atom la-2x">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
    <span id="config-title" style="display:none">Alpha Hinex&#39;s Blog</span>
    <div id="loader"></div>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation animated fadeIn fast delay-1s">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="https://AlphaHinex.github.io">
        <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div>
        <h3 class="subtitle">【译】可视化神经机器翻译模型（Seq2seq 模型的注意力机制）</h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                        <a href="javascript:;" class="iconfont icon-share1"></a>
                        <div class="share-component-cc" data-disabled="facebook,douban,linkedin,diandian,tencent,google"></div>
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class=article-header-wrapper>
    <div class="article-header">
        <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/contents/attention/attention.png);">
        </div>
        <div class="else">
            <p class="animated fadeInDown">
                
                <a href="/categories/AI"><b>「
                    </b>AI<b> 」</b></a>
                
                九月 17, 2023
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/2023/09/17/attention/" title="【译】可视化神经机器翻译模型（Seq2seq 模型的注意力机制）" class="">【译】可视化神经机器翻译模型（Seq2seq 模型的注意力机制）</a>
            </h3>
            
            <p class="post-count animated fadeInDown">
                
                <span>
                    <b class="iconfont icon-text2"></b> <i>文章字数</i>
                    10k
                </span>
                
                
                <span>
                    <b class="iconfont icon-timer__s"></b> <i>阅读约需</i>
                    9 mins.
                </span>
                
                
                
                <span id="busuanzi_page_pv_container" style="display: flex;">
                    <b class="iconfont icon-read"></b> <i>阅读次数</i>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                
            </p>
            
            
            <ul class="animated fadeInDown post-tags-list" itemprop="keywords"><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/AI/" rel="tag">AI</a></li></ul>
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn">
            <ul>
<li>原文地址：<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></li>
<li>作者：<a href="https://jalammar.github.io/" target="_blank" rel="noopener">Jay Alammar</a></li>
</ul>
<style>
.encoder{
  color:#70BF41
}

.decoder{
  color:#B36AE2
}

.context{
  color:#F39019
}

.embedding{
  color:#00882B
}


.step_no{
  color:#5CBCE9
}

.ffnn{
  color:#EC5D57
}

.logits_output{
  color:#DF5F91
}

.img-div {
  max-width: 600px;
  margin: auto;
  font-size: 85%;
  color: #999;
}

.img-div img {
  border: 1px solid #eee;
}
</style>

<p><strong>注意：</strong> 下面的动画是视频。轻触或（使用鼠标）悬停在它们上，可获得播放控件，以便在需要时暂停。</p>
<p>序列到序列（Sequence-to-sequence）模型是一种深度学习模型，在诸如机器翻译、文本摘要和图像标题生成等任务中取得了许多成功。Google Translate 在 2016 年底开始在生产环境中 <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/" target="_blank" rel="noopener">使用</a> 这种模型。这些模型在两篇开创性论文（<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever et al., 2014</a>, <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" target="_blank" rel="noopener">Cho et al., 2014</a>）中进行了说明。</p>
<p>然而我发现，充分理解并实现模型需要逐步揭示一系列相互依存的概念。我认为这些概念以视觉方式表达会更加易于理解。这就是我在本文试图做的。阅读本文需要对深度学习有一定的了解。我希望本文可以成为您阅读上面提到的论文（和本文之后引用的注意力论文）的得力助手。</p>
<p>序列到序列模型是一种将一系列项目（如单词、字母、图像的特征等）输入并输出一系列其他项目的模型。一个训练过的模型将以如下方式工作：</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>在神经机器翻译中，序列是一系列单词，逐个处理后，输出也是一系列单词：</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_2.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<h2 id="深入了解"><a href="#深入了解" class="headerlink" title="深入了解"></a>深入了解</h2><p>在内部，该模型由一个 编码器 <span class="encoder">encoder</span> 和一个 解码器 <span class="decoder">decoder</span> 组成。</p>
<p>编码器 <span class="encoder">encoder</span> 处理输入序列中的每个项目，并将其捕获的信息编译成一个向量（称为 上下文 <span class="context">context</span>）。在处理完整个输入序列之后，编码器 <span class="encoder">encoder</span> 将上下文 <span class="context">context</span> 发送给解码器 <span class="decoder">decoder</span>，解码器开始逐个生成输出序列的项目。</p>
<video width="100%" height="auto" loop autoplay  controls>
  <source src="https://jalammar.github.io/images/seq2seq_3.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>机器翻译也是相同的情况。</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_4.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>在机器翻译中，<span class="context">context</span> 是一个向量（基本上是由数字组成的数组）。编码器 <span class="encoder">encoder</span> 和解码器 <span class="decoder">decoder</span> 通常都是循环神经网络（recurrent neural networks，RNN）。请务必查看 Luis Serrano 的 <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target="_blank" rel="noopener">A friendly introduction to Recurrent Neural Networks</a> 以了解 RNN 的基础知识。</p>
<div class="img-div" markdown="0">
    <img src="/contents/attention/context.png" />
    上下文 <span class="context">context</span> 是浮点数组成的向量。稍后我们在本文中将以颜色可视化向量，将更高值的单元格分配更亮的颜色。
</div>

<p>在设置您的模型时，您可以设置上下文 <span class="context">context</span> 向量的大小。基本上，这是在 RNN 编码器 <span class="encoder">encoder</span> 中的隐藏单元数量。这些可视化展示了一个大小为 4 的向量，但在现实应用中，上下文 <span class="context">context</span> 向量大小可能会是 256、512 或 1024 等。</p>
<p>按设计，RNN 在每个时间步骤中接受两个输入：一个输入（在编码器的情况下，是输入句子中的一个单词）和一个隐藏状态。并且，单词需要用向量来表示。为了将单词转化为向量，我们使用被称为 “词嵌入 <a href="https://machinelearningmastery.com/what-are-word-embeddings/" target="_blank" rel="noopener">word embedding</a>” 算法的方法类。这些算法将单词转化为向量空间，其中包含单词的许多含义/语义信息（例如：<a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html" target="_blank" rel="noopener">king - man + woman = queen</a>）。</p>
<div class="img-div" markdown="0">
    <img src="/contents/attention/embedding.png" />
    在处理输入之前，我们需要将输入单词转换为向量。这个过程使用 <a href="https://en.wikipedia.org/wiki/Word_embedding" target="_blank" rel="noopener">word embedding</a> 算法完成。我们可以使用预训练的嵌入 <a href="http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/" target="_blank" rel="noopener">pre-trained embeddings</a> 或在我们自己的数据集上训练自己的嵌入。典型的嵌入向量边长为 200 或 300，为了简单起见，我们在这里展示了一个大小为 4 的向量。
</div>

<p>现在我们已经介绍了我们的主要向量/张量，让我们回顾一下 RNN 的机制并建立一个可视化的语言来描述这些模型：</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/RNN_1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>RNN 下一步会使用第二个输入向量和第一步的隐藏状态，创建第二个时间步骤的输出。</p>
<p>下一步是使用第二个输入向量和一个隐藏状态#1来创建该时间的输出。后文中，我们会使用类似这样的动画来描述神经机器翻译模型内部的向量。</p>
<p>在接下来的可视化中，每个编码器 <span class="encoder">encoder</span> 或解码器 <span class="decoder">decoder</span> 的脉冲都是 RNN 处理其输入并生成该时间步的输出。由于编码器 <span class="encoder">encoder</span> 和解码器 <span class="decoder">decoder</span> 都是 RNN，每个时间步骤中一个 RNN 进行一些处理，它根据它的输入和之前步骤中它能看到的输入来更新其隐藏状态 <span class="context">hidden state</span>。</p>
<p>让我们观察编码器 <span class="encoder">encoder</span> 的隐藏状态 <span class="context">hidden states</span>。请注意，实际上最后一个隐藏状态 <span class="context">hidden state</span> 我们作为上下文 <span class="context">context</span>传递给了解码器 <span class="decoder">decoder</span>。</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_5.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>在解码器 <span class="decoder">decoder</span> 中，也会维护一个从一个时间步骤传递到下一个时间步骤的隐藏状态 <span class="decoder">hidden state</span>。但我们目前关注的是模型的主要部分，所以没有在可视化中展示它。</p>
<p>现在让我们来看看另一种可视化序列到序列模型的方式。这个动画将更容易理解描述这些模型的静态图形。这被称为“展开”视图，在这个视图中，我们不只显示一个解码器 <span class="decoder">decoder</span>，而是为每个时间步骤显示一个副本。这样我们就可以查看每个时间步骤的输入和输出。</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_6.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<h2 id="让我们集中注意力"><a href="#让我们集中注意力" class="headerlink" title="让我们集中注意力"></a>让我们集中注意力</h2><p>对于这类模型来说，上下文 <span class="context">context</span> 向量成为了一个瓶颈。它使得模型难以处理较长的句子。<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Bahdanau et al., 2014</a> 和 <a href="https://arxiv.org/abs/1508.04025" target="_blank" rel="noopener">Luong et al., 2015</a> 提出了一个解决方案。这些论文引入并改进了一种被称为“注意力”的技术，极大地提高了机器翻译系统的质量。注意力允许模型根据需要专注于输入序列的相关部分。</p>
<img src="/contents/attention/attention.png" />

<div class="img-div" markdown="0">
    在时间步骤 7 中，注意力机制使解码器 <span class="decoder">decoder</span> 能够在生成英语翻译之前关注输入序列中的 “étudiant”（法语中的 “student”）。这种在输入序列相关部分放大信号的能力使得注意力模型产生的结果优于没有注意力的模型。
</div>

<p>让我们继续在这个高层抽象层面上查看注意力模型。注意力模型与经典的序列到序列模型有两个主要区别：</p>
<p>首先，编码器 <span class="encoder">encoder</span> 将更多的数据传递给解码器 <span class="decoder">decoder</span>。编码器 <span class="encoder">encoder</span> 不再只传递编码阶段的最后隐藏状态，而是将 <em>所有</em> 隐藏状态 <span class="context">hidden states</span> 都传递给解码器 <span class="decoder">decoder</span>：</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_7.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>其次，在生成输出之前，有注意力机制的解码器 <span class="decoder">decoder</span> 会执行一个额外的步骤。为了聚焦于与当前解码时间步骤相关的输入部分，解码器 <span class="decoder">decoder</span> 会执行以下操作：</p>
<ol>
<li>浏览它收到的编码器隐藏状态 <span class="context">hidden states</span> 集合 - 每个编码器隐藏状态 <span class="context">encoder hidden state</span> 与输入句子中最相关的某个单词关联</li>
<li>给每个隐藏状态 <span class="context">hidden state</span> 赋予一个分数（我们先不考虑分数的计算方式）</li>
<li>将每个隐藏状态 <span class="context">hidden state</span> 乘以其经过 softmax 处理后的分数，从而放大具有高分数的隐藏状态 <span class="context">hidden states</span>，并淹没具有低分数的隐藏状态 <span class="context">hidden states</span></li>
</ol>
<video width="100%" height="auto" loop autoplay controls>
   <source src="https://jalammar.github.io/images/attention_process.mp4" type="video/mp4">
   Your browser does not support the video tag.
</video>

<p>这个评分过程在解码器 <span class="decoder">decoder</span> 端的每个时间步骤中进行。</p>
<p>现在让我们将所有内容整合到以下可视化中，看看注意力过程是如何工作的：</p>
<ol>
<li>注意力解码器 RNN 接收 <span class="embedding">&lt;END&gt;</span> 符号的嵌入向量，和一个初始解码器隐藏状态 <span class="decoder">initial decoder hidden state</span>。</li>
<li>RNN 处理输入，生成一个输出和一个新的隐藏状态 <span class="decoder">new hidden state</span> 向量（<span class="decoder">h</span><span class="step_no">4</span>）。输出被丢弃。</li>
<li>注意力步骤：我们使用编码器隐藏状态 <span class="context">encoder hidden states</span> 和 <span class="decoder">h</span><span class="step_no">4</span> 向量来计算该时间步骤的上下文向量（<span class="step_no">C</span><span class="decoder">4</span>）。</li>
<li>我们将 <span class="decoder">h</span><span class="step_no">4</span> 和 <span class="step_no">C</span><span class="decoder">4</span> 连接成一个向量。</li>
<li>我们通过一个前馈神经网络 <span class="ffnn">feedforward neural network</span>（与模型一起训练的网络）传递这个向量。</li>
<li>前馈神经网络的输出 <span class="logits_output">output</span> 指示了该时间步骤的输出单词。</li>
<li>下一个时间步骤重复以上步骤。</li>
</ol>
<video width="100%" height="auto" loop autoplay controls>
   <source src="https://jalammar.github.io/images/attention_tensor_dance.mp4" type="video/mp4">
   Your browser does not support the video tag.
</video>

<p>这是另一种观察我们在每个解码步骤上关注输入句子的哪个部分的方式：</p>
<video width="100%" height="auto" loop autoplay controls>
  <source src="https://jalammar.github.io/images/seq2seq_9.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p>请注意，模型并不是简单地将输出的第一个单词与输入的第一个单词对齐。它实际上是在训练阶段学习了如何对齐该语言对（例如上例中的法语和英语）。可以从上面列出的注意力论文中看到这种机制有多么精确的例子：</p>
<div class="img-div" markdown="0">
<img src="/contents/attention/attention_sentence.png" />
    您可以看到，当输出“European Economic Area”时，模型正确地进行了注意力对齐。在法语中，这些单词的顺序与英语相反（“européenne économique zone”）。句子中的其他单词也以类似的顺序排列。
</div>

<p>如果您觉得已经准备好学习实现的内容，请务必查看 TensorFlow 的 <a href="https://github.com/tensorflow/nmt" target="_blank" rel="noopener">Neural Machine Translation (seq2seq) Tutorial</a>。</p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls"
                data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
            <ul id="audio-list" style="display:none">
                
                
                <li title='0' data-url='/statics/background.mp3'></li>
                
                    
            </ul>
            
            
            
    <div id='gitalk-container' class="comment link"
        data-ae='true'
        data-ci='54f9966b8cc9d2423ffd'
        data-cs='504574e4532bdfa77d3e4091637ff53558408ac2'
        data-r='AlphaHinex.github.io'
        data-o='AlphaHinex'
        data-a='AlphaHinex'
        data-d=''
    >留言</div>


            
            
        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <img src="/img/hinex.jpg" height=300 width=300></img>
                    <p>Alpha Hinex</p>
                    <span>Stay Hungry. Stay Foolish.</span>
                    <dl>
                        <dd><a href="https://github.com/AlphaHinex" target="_blank"><span
                                    class=" iconfont icon-github"></span></a></dd>
                        <dd><a href="/whoami"><span class=" iconfont icon-wechat"></span></a></dd>
                        <dd><a href="" target="_blank"><span
                                    class=" iconfont icon-stack-overflow"></span></a></dd>
                    </dl>
                </div>
                <ul>
                    <li><a href="/">314 <p>文章</p></a></li>
                    <li><a href="/categories">76 <p>分类</p></a></li>
                    <li><a href="/tags">153 <p>标签</p></a></li>
                </ul>
            </div>
            
            
            
            <div class="box sticky animated fadeInRight faster">
                <div id="toc" class="subbox">
                    <h4>目录</h4>
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#深入了解"><span class="toc-number">1.</span> <span class="toc-text">深入了解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#让我们集中注意力"><span class="toc-number">2.</span> <span class="toc-text">让我们集中注意力</span></a></li></ol>
                </div>
            </div>
            
            
        </div>
    </div>
</div>

    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p>本站访客数<span id="busuanzi_value_site_uv"></span>人次 / 本站总访问量<span id="busuanzi_value_site_pv"></span>次</p>
    <p class="copyright" id="copyright">
        &copy; 2025
        <span class="gradient-text">
            Alpha Hinex
        </span>.
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
        Theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.3" target="_blank" rel="noopener">v1.4.3</a></small>
    </p>
</footer>

<script type="text/javascript" src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX", "TeX"],
      linebreaks: {
        automatic: true
      },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"]
      ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      noUndefined: {
        attributes: {
          mathcolor: "red",
          mathbackground: "#FFEEEE",
          mathsize: "90%"
        }
      },
      Macros: {
        href: "{}"
      }
    },
    messageStyle: "none"
  });
</script>
<script>
  function initialMathJax() {
    MathJax.Hub.Queue(function () {
      var all = MathJax.Hub.getAllJax(),
        i;
      // console.log(all);
      for (i = 0; i < all.length; i += 1) {
        console.log(all[i].SourceElement().parentNode)
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  }

  function reprocessMathJax() {
    if (typeof MathJax !== 'undefined') {
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    }
  }
</script>



    
<link rel="stylesheet" href="/css/gitalk.css">

    
<script src="/js/gitalk.min.js"></script>



<script src="/js/jquery-3.4.1.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/typed.js/2.0.10/typed.min.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/blueimp-md5/2.12.0/js/md5.min.js"></script>


<script src="/js/social-share.min.js"></script>


<script src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.js"></script>

    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/javascript/javascript.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/css/css.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/xml/xml.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/clike/clike.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/php/php.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/shell/shell.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/python/python.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/groovy/groovy.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/diff/diff.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/nginx/nginx.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/properties/properties.min.js"></script>




    
<script src="/js/busuanzi.min.js"></script>

    <script>
        $(document).ready(function () {
            if ($('span[id^="busuanzi_"]').length) {
                initialBusuanzi();
            }
        });
    </script>



<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-69084811-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-69084811-1');
    </script>





<script>
    function initialTyped () {
        var typedTextEl = $('.typed-text');
        if (typedTextEl && typedTextEl.length > 0) {
            var typed = new Typed('.typed-text', {
                strings: ["Stay Hungry. Stay Foolish.", "常与同好争高下，莫与傻子论短长"],
                typeSpeed: 90,
                loop: true,
                loopCount: Infinity,
                backSpeed: 20,
            });
        }
    }

    if ($('.article-header') && $('.article-header').length) {
        $(document).ready(function () {
            initialTyped();
        });
    }
</script>




</html>
