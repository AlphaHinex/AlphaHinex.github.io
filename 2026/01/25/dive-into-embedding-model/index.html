
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>浅析 Embedding 模型 - Alpha Hinex&#39;s Blog</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Java, JavaScript, Spring, Html5, NoSQL, Docker, DevOps,MedImageInsight, lion-ai, text encoder, lang_encoder, weight, image encoder, transformer, forward, permute, multihead attention, token, tokenizer, model, normalization"> 
    <meta name="description" content="在 MedImageInsight 中，介绍了由微软发表，第三方发布的医学图像嵌入模型，本文将以其中的 Text Encoder 为例，浅析 Embedding 模型的工作原理。
什么是 Embed,"> 
    <meta name="author" content="Alpha Hinex"> 

    <meta name="msvalidate.01" content="D769824B4D44C14A4C777A6EC4E898FC"> 
    <meta name="baidu-site-verification" content="TimiEK4Y9V"> 
    <meta name="google-site-verification" content="B-WME81HWSnMIkZZxcxv7bVI6yjbpAFKvifi2X-EkzQ"> 

    <link rel="alternative" href="atom.xml" title="Alpha Hinex&#39;s Blog" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link href="https://fonts.loli.net/css?family=Roboto+Mono|Rubik&display=swap" rel="stylesheet">
    
<link rel="stylesheet" href="//at.alicdn.com/t/font_1429596_nzgqgvnmkjb.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/animate.css/3.7.2/animate.min.css">

    
<link rel="stylesheet" href="/css/share.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.css">

    
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/theme/dracula.css">

    
<link rel="stylesheet" href="/css/obsidian.css">

    
<link rel="stylesheet" href="/css/ball-atom.min.css">

<meta name="generator" content="Hexo 4.2.1"></head>


<body class="loading">
    <div class="loader">
        <div class="la-ball-atom la-2x">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
    <span id="config-title" style="display:none">Alpha Hinex&#39;s Blog</span>
    <div id="loader"></div>
    <div id="single">
    <div class="scrollbar gradient-bg-rev"></div>
<div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <div class="navigation animated fadeIn fast delay-1s">
        <img id="home-icon" class="icon-home" src="/img/favicon.png" alt="" data-url="https://AlphaHinex.github.io">
        <div id="play-icon" title="Play/Pause" class="iconfont icon-play"></div>
        <h3 class="subtitle">浅析 Embedding 模型</h3>
        <div class="social">
            <!--        <div class="like-icon">-->
            <!--            <a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
            <!--        </div>-->
            <div>
                <div class="share">
                    
                        <a href="javascript:;" class="iconfont icon-share1"></a>
                        <div class="share-component-cc" data-disabled="facebook,douban,linkedin,diandian,tencent,google"></div>
                    
                </div>
            </div>
        </div>
    </div>
</div>

    <div class="section">
        <div class=article-header-wrapper>
    <div class="article-header">
        <div class="article-cover animated fadeIn" style="
            animation-delay: 600ms;
            animation-duration: 1.2s;
            background-image: 
                radial-gradient(ellipse closest-side, rgba(0, 0, 0, 0.65), #100e17),
                url(/contents/dive-into-embedding-model/lang_encoder.png);">
        </div>
        <div class="else">
            <p class="animated fadeInDown">
                
                <a href="/categories/AI"><b>「
                    </b>AI<b> 」</b></a>
                
                一月 25, 2026
            </p>
            <h3 class="post-title animated fadeInDown"><a href="/2026/01/25/dive-into-embedding-model/" title="浅析 Embedding 模型" class="">浅析 Embedding 模型</a>
            </h3>
            
            <p class="post-count animated fadeInDown">
                
                <span>
                    <b class="iconfont icon-text2"></b> <i>文章字数</i>
                    11k
                </span>
                
                
                <span>
                    <b class="iconfont icon-timer__s"></b> <i>阅读约需</i>
                    10 mins.
                </span>
                
                
                
                <span id="busuanzi_page_pv_container" style="display: flex;">
                    <b class="iconfont icon-read"></b> <i>阅读次数</i>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                
            </p>
            
            
            <ul class="animated fadeInDown post-tags-list" itemprop="keywords"><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Embedding-Model/" rel="tag">Embedding Model</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Medical-Imaging/" rel="tag">Medical Imaging</a></li><li class="animated fadeInDown post-tags-list-item"><a class="animated fadeInDown post-tags-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul>
            
        </div>
    </div>
</div>

<div class="screen-gradient-after">
    <div class="screen-gradient-content">
        <div class="screen-gradient-content-inside">
            <div class="bold-underline-links screen-gradient-sponsor">
                <p>
                    <span class="animated fadeIn delay-1s"></span>
                </p>
            </div>
        </div>
    </div>
</div>

<div class="article">
    <div class='main'>
        <div class="content markdown animated fadeIn">
            <p>在 <a href="https://alphahinex.github.io/2025/12/21/medimageinsight/">MedImageInsight</a> 中，介绍了由微软发表，第三方发布的医学图像嵌入模型，本文将以其中的 Text Encoder 为例，浅析 Embedding 模型的工作原理。</p>
<h2 id="什么是-Embedding-模型"><a href="#什么是-Embedding-模型" class="headerlink" title="什么是 Embedding 模型"></a>什么是 Embedding 模型</h2><p>Embedding 模型是一种将高维数据（如文本、图像等）转换为低维向量表示的模型。通过这种转换，模型能够捕捉数据的语义信息，使得相似的数据在向量空间中距离更近。</p>
<p>以 <a href="https://huggingface.co/lion-ai/MedImageInsights" target="_blank" rel="noopener">lion-ai/MedImageInsights</a> 中的 Text Encoder 为例（其使用的分词器只支持英文），输入 <code>[&quot;lumbar spine&quot;, &quot;chest&quot;, &quot;desktop&quot;]</code> ，将得到如下的向量表示：</p>
<pre><code class="python"># text_embeddings: {ndarray: (2, 1024)}
[
    [ -0.03271601,  0.02388754,  0.02584449, ...,  0.00670768,  0.00604417, -0.01052989],
    [ -0.03645158, -0.00188246,  0.00022278, ..., -0.02536389, -0.02322166,  0.02286733],
    [ -0.0056968 ,  0.06878266,  0.0043339 , ..., -0.02845573,  0.01347476, -0.04035829]
]</code></pre>
<p>每个输入文本被转换为一个 1024 维的向量。计算这些向量的余弦相似度，可以发现 <code>lumbar spine</code> 和 <code>chest</code> 的相似度较高，而 <code>desktop</code> 与前两者的相似度较低，这反映了它们在语义上的关系：</p>
<pre><code class="python">lumbar spine, chest 余弦相似度:  0.728603
lumbar spine, desktop 余弦相似度:  0.33807534
chest, desktop 余弦相似度:  0.38285893</code></pre>
<h2 id="lion-ai-MedImageInsights-中的-Text-Encoder-是怎么把文本转换为向量的"><a href="#lion-ai-MedImageInsights-中的-Text-Encoder-是怎么把文本转换为向量的" class="headerlink" title="lion-ai/MedImageInsights 中的 Text Encoder 是怎么把文本转换为向量的"></a>lion-ai/MedImageInsights 中的 Text Encoder 是怎么把文本转换为向量的</h2><p>简单来说：</p>
<ol>
<li>文本先经过分词器处理成 token 序列；</li>
<li>之后输入到 Text Encoder 模型中，通过其架构和权重进行前向传播，将 token 序列转换为向量表示；</li>
<li>最后进行投影（为了将文本编码器和图像编码器的输出投影到同一个嵌入空间）和向量归一化。</li>
</ol>
<p>下面以输入 <code>[&quot;lumbar spine&quot;, &quot;chest&quot;]</code> 为例，推演文本转换成向量的过程。</p>
<h3 id="分词器（Tokenizer）"><a href="#分词器（Tokenizer）" class="headerlink" title="分词器（Tokenizer）"></a>分词器（Tokenizer）</h3><p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/medimageinsightmodel.py#L50-L55" target="_blank" rel="noopener">load_model</a> 时，设置分词器路径为 <code>&#39;2024.09.27/language_model/clip_tokenizer_4.16.2&#39;</code>：</p>
<pre><code class="python"># Set paths
self.opt[&#39;LANG_ENCODER&#39;][&#39;PRETRAINED_TOKENIZER&#39;] = os.path.join(
    self.model_dir,
    &#39;language_model&#39;,
    &#39;clip_tokenizer_4.16.2&#39;
)</code></pre>
<p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/LangEncoder/build.py#L37-L41" target="_blank" rel="noopener">build_tokenizer</a> 中，使用 <code>CLIPTokenizer</code> 加载分词器：</p>
<pre><code class="python">pretrained_tokenizer = config_encoder.get(
    &#39;PRETRAINED_TOKENIZER&#39;, &#39;openai/clip-vit-base-patch32&#39;
)
tokenizer = CLIPTokenizer.from_pretrained(pretrained_tokenizer)</code></pre>
<p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/medimageinsightmodel.py#L179-L185" target="_blank" rel="noopener">encode</a> 中，调用分词器对文本进行分词：</p>
<pre><code class="python">text_tokens = self.tokenize(
    texts,
    padding=&#39;max_length&#39;,
    max_length=self.max_length,
    truncation=True,
    return_tensors=&#39;pt&#39;
)</code></pre>
<p>得到的 token ID 可在 <a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/2024.09.27/language_model/clip_tokenizer_4.16.2/vocab.json" target="_blank" rel="noopener">vocab.json</a> 中查找对应的词汇。</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/tokenize.png" alt="tokenize"></p>
<h3 id="模型架构（Model-Architecture）"><a href="#模型架构（Model-Architecture）" class="headerlink" title="模型架构（Model Architecture）"></a>模型架构（Model Architecture）</h3><p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/UniCLModel.py#L293" target="_blank" rel="noopener">build_unicl_model</a> 处可以观察到模型的架构（<code>image_encoder</code> 部分省略）：</p>
<pre><code class="python">UniCLModel(
  (lang_encoder): Transformer(
    (token_embedding): Embedding(49408, 1024)
    (resblocks): ModuleList(
      (0-15): 16 x ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (ln_1): LayerNorm()
        (mlp): Sequential(
          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
        )
        (ln_2): LayerNorm()
        (drop_path): Identity()
      )
    )
    (ln_final): LayerNorm()
  )
  (image_encoder): DaViT(
    (convs): ModuleList(
      (0): ConvEmbed(...)
      (1): ConvEmbed(...)
      (2): ConvEmbed(...)
      (3): ConvEmbed(...)
    )
    (blocks): ModuleList(
      (0): MySequential(
        (0): MySequential(...)
      )
      (1): MySequential(
        (0): MySequential(...)
      )
      (2): MySequential(
        (0): MySequential(...)
        (1): MySequential(...)
        (2): MySequential(...)
        (3): MySequential(...)
        (4): MySequential(...)
        (5): MySequential(...)
        (6): MySequential(...)
        (7): MySequential(...)
        (8): MySequential(...)
      )
      (3): MySequential(
        (0): MySequential(...)
      )
    )
    (norms): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
)</code></pre>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/lang_encoder.png" alt="lang_encoder"></p>
<h3 id="模型权重（Model-Weights）"><a href="#模型权重（Model-Weights）" class="headerlink" title="模型权重（Model Weights）"></a>模型权重（Model Weights）</h3><p>尽管 lion-ai 发布的模型权重文件 <a href="https://huggingface.co/lion-ai/MedImageInsights/blob/main/2024.09.27/vision_model/medimageinsigt-v1.0.0.pt" target="_blank" rel="noopener">2024.09.27/vision_model/medimageinsigt-v1.0.0.pt</a> 扩展名是 <code>pt</code>，但实际是 <a href="https://github.com/huggingface/safetensors" target="_blank" rel="noopener">Safetensors</a> 格式，在 <a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/UniCLModel.py#L95-L96" target="_blank" rel="noopener">from_pretrained</a> 中读取：</p>
<pre><code class="python">from safetensors.torch import load_file

## Load SafeTensors Version of Pretrained Model
pretrained_dict = load_file(pretrained)</code></pre>
<p>其中包含张量如下，与上面模型架构一一对应：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/safetensors.png" alt="safetensors"></p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>继续上面文本嵌入向量的过程。在经过分词器处理后，得到的 token ID 序列会经过 Text Encoder 模型的 Transformer 结构进行前向传播。该 Transformer 主要包括三部分：</p>
<ol>
<li><strong>Embedding 层</strong>：将 token ID 转换为初始的向量表示。</li>
<li><strong>多层 ResidualAttentionBlock</strong>：通过自注意力机制和前馈网络，捕捉序列中的上下文信息。</li>
<li><strong>LayerNorm 层</strong>：对输出进行归一化处理，稳定训练过程。</li>
</ol>
<h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/LangEncoder/transformer.py#L183-L185" target="_blank" rel="noopener">forward</a> 方法使用 token ID 序列从模型权重文件中的 <code>lang_encoder.token_embedding.weight</code> 中查找对应的嵌入向量：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/token_embedding.png" alt="token_embedding"></p>
<p>之后与位置嵌入矩阵相加：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/positional_embedding.png" alt="positional_embedding"></p>
<p>由于 <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer.forward" target="_blank" rel="noopener">Transformer.forward()</a> 对于输入张量的形状要求是：</p>
<blockquote>
<p><strong>Shape:</strong></p>
<ul>
<li>src: (S,E) for unbatched input, (S,N,E) if batch_first=False or (N, S, E) if batch_first=True.</li>
</ul>
</blockquote>
<blockquote>
<p>where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number</p>
</blockquote>
<p>故将三维张量由 <code>(N, L, D)</code> 重新排列为 <code>(L, N, D)</code>，供后续的多层 ResidualAttentionBlock 处理。</p>
<blockquote>
<ul>
<li>N: 批量大小（Batch Size）</li>
<li>L: 序列长度（Sequence Length）</li>
<li>D: 嵌入维度（Embedding Dimension）</li>
</ul>
</blockquote>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/permute.png" alt="permute"></p>
<h4 id="ResidualAttentionBlock"><a href="#ResidualAttentionBlock" class="headerlink" title="ResidualAttentionBlock"></a>ResidualAttentionBlock</h4><p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/LangEncoder/transformer.py#L107-L112" target="_blank" rel="noopener">resblocks</a> 由 16 个 <a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/LangEncoder/transformer.py#L44-L78" target="_blank" rel="noopener">ResidualAttentionBlock</a> 组成，每个块包括以下组件：</p>
<ol>
<li><strong>MultiheadAttention</strong>：多头注意力层，允许模型关注输入序列的不同部分。</li>
<li><strong>LayerNorm</strong>：对输入进行归一化，稳定训练过程。</li>
<li><strong>MLP（多层感知机，前馈神经网络）</strong>：包含两个线性层和一个激活函数（<a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/LangEncoder/transformer.py#L39-L41" target="_blank" rel="noopener">QuickGELU</a>），用于进一步处理注意力输出。</li>
<li><strong>LayerNorm</strong>：再次对输出进行归一化。</li>
<li><strong>Drop Path</strong>：根据参数决定使用 DropPath（随机丢弃路径）或 Identity（无操作）。</li>
</ol>
<p>经过多层 <code>ResidualAttentionBlock</code> 处理后，张量形状仍为 <code>(L, N, D)</code>：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/resblocks.png" alt="resblocks"></p>
<p>将其重新排列成 <code>(N, L, D)</code>：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/permute_back.png" alt="permute_back"></p>
<h4 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a>LayerNorm</h4><p>最后归一化处理：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/ln_final.png" alt="ln_final"></p>
<p>此时张量的形状是 <code>(2, 77, 1024)</code>，按批次提取 token ID 序列中第一个最大值（即 <code>EOF</code>，<code>49407</code>）位置索引的向量，得到 <code>(2, 1024)</code> 形状的张量，即为输入文本嵌入后的结果：</p>
<p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/UniCLModel.py#L174C13-L174C77" target="_blank" rel="noopener"><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/clip.png" alt="clip"></a></p>
<h3 id="投影（Projection）"><a href="#投影（Projection）" class="headerlink" title="投影（Projection）"></a>投影（Projection）</h3><p>在完成上面的计算后，<code>MedImageInsight</code> 还需要进行一次投影操作，用以将 <code>Image Encoder</code> 和 <code>Text Encoder</code> 的编码结果投影到同一个嵌入空间，以便能够判断文字和图片的语义相似度。</p>
<p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/UniCLModel.py#L178" target="_blank" rel="noopener"><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/projection.png" alt="projection"></a></p>
<h3 id="向量归一化（Vector-Normalization）"><a href="#向量归一化（Vector-Normalization）" class="headerlink" title="向量归一化（Vector Normalization）"></a>向量归一化（Vector Normalization）</h3><p><a href="https://github.com/AlphaHinex/MedImageInsights/blob/develop/MedImageInsight/UniCLModel.py#L180-L181" target="_blank" rel="noopener">encode_text</a> 将投影后的向量再次归一化后返回，得到最终结果：</p>
<p><img src="https://alphahinex.github.io/contents/dive-into-embedding-model/output.png" alt="output"></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls"
                data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
            <ul id="audio-list" style="display:none">
                
                
                <li title='0' data-url='/statics/background.mp3'></li>
                
                    
            </ul>
            
            
            
    <div id='gitalk-container' class="comment link"
        data-ae='true'
        data-ci='54f9966b8cc9d2423ffd'
        data-cs='504574e4532bdfa77d3e4091637ff53558408ac2'
        data-r='AlphaHinex.github.io'
        data-o='AlphaHinex'
        data-a='AlphaHinex'
        data-d=''
    >留言</div>


            
            
        </div>
        <div class="sidebar">
            <div class="box animated fadeInRight">
                <div class="subbox">
                    <img src="/img/hinex.jpg" height=300 width=300></img>
                    <p>Alpha Hinex</p>
                    <span>Stay Hungry. Stay Foolish.</span>
                    <dl>
                        <dd><a href="https://github.com/AlphaHinex" target="_blank"><span
                                    class=" iconfont icon-github"></span></a></dd>
                        <dd><a href="/whoami"><span class=" iconfont icon-wechat"></span></a></dd>
                        <dd><a href="" target="_blank"><span
                                    class=" iconfont icon-stack-overflow"></span></a></dd>
                    </dl>
                </div>
                <ul>
                    <li><a href="/">331 <p>文章</p></a></li>
                    <li><a href="/categories">78 <p>分类</p></a></li>
                    <li><a href="/tags">159 <p>标签</p></a></li>
                </ul>
            </div>
            
            
            
            <div class="box sticky animated fadeInRight faster">
                <div id="toc" class="subbox">
                    <h4>目录</h4>
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是-Embedding-模型"><span class="toc-number">1.</span> <span class="toc-text">什么是 Embedding 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lion-ai-MedImageInsights-中的-Text-Encoder-是怎么把文本转换为向量的"><span class="toc-number">2.</span> <span class="toc-text">lion-ai&#x2F;MedImageInsights 中的 Text Encoder 是怎么把文本转换为向量的</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分词器（Tokenizer）"><span class="toc-number">2.1.</span> <span class="toc-text">分词器（Tokenizer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型架构（Model-Architecture）"><span class="toc-number">2.2.</span> <span class="toc-text">模型架构（Model Architecture）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型权重（Model-Weights）"><span class="toc-number">2.3.</span> <span class="toc-text">模型权重（Model Weights）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-number">2.4.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#投影（Projection）"><span class="toc-number">2.5.</span> <span class="toc-text">投影（Projection）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#向量归一化（Vector-Normalization）"><span class="toc-number">2.6.</span> <span class="toc-text">向量归一化（Vector Normalization）</span></a></li></ol></li></ol>
                </div>
            </div>
            
            
        </div>
    </div>
</div>

    </div>
</div>
    <div id="back-to-top" class="animated fadeIn faster">
        <div class="flow"></div>
        <span class="percentage animated fadeIn faster">0%</span>
        <span class="iconfont icon-top02 animated fadeIn faster"></span>
    </div>
</body>
<footer>
    <p>本站访客数<span id="busuanzi_value_site_uv"></span>人次 / 本站总访问量<span id="busuanzi_value_site_pv"></span>次</p>
    <p class="copyright" id="copyright">
        &copy; 2026
        <span class="gradient-text">
            Alpha Hinex
        </span>.
        Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
        Theme
        <span class="gradient-text">
            <a href="https://github.com/TriDiamond/hexo-theme-obsidian" title="Obsidian" target="_blank" rel="noopener">Obsidian</a>
        </span>
        <small><a href="https://github.com/TriDiamond/hexo-theme-obsidian/blob/master/CHANGELOG.md" title="v1.4.3" target="_blank" rel="noopener">v1.4.3</a></small>
    </p>
</footer>

<script type="text/javascript" src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX", "TeX"],
      linebreaks: {
        automatic: true
      },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"]
      ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      noUndefined: {
        attributes: {
          mathcolor: "red",
          mathbackground: "#FFEEEE",
          mathsize: "90%"
        }
      },
      Macros: {
        href: "{}"
      }
    },
    messageStyle: "none"
  });
</script>
<script>
  function initialMathJax() {
    MathJax.Hub.Queue(function () {
      var all = MathJax.Hub.getAllJax(),
        i;
      // console.log(all);
      for (i = 0; i < all.length; i += 1) {
        console.log(all[i].SourceElement().parentNode)
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  }

  function reprocessMathJax() {
    if (typeof MathJax !== 'undefined') {
      MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
    }
  }
</script>



    
<link rel="stylesheet" href="/css/gitalk.css">

    
<script src="/js/gitalk.min.js"></script>



<script src="/js/jquery-3.4.1.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/obsidian.js"></script>
<script src="/js/jquery.truncate.js"></script>
<script src="/js/search.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/typed.js/2.0.10/typed.min.js"></script>


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/blueimp-md5/2.12.0/js/md5.min.js"></script>


<script src="/js/social-share.min.js"></script>


<script src="https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/codemirror.min.js"></script>

    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/javascript/javascript.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/css/css.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/xml/xml.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/htmlmixed/htmlmixed.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/clike/clike.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/php/php.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/shell/shell.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/python/python.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/groovy/groovy.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/diff/diff.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/nginx/nginx.min.js"></script>


    
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/codemirror/5.48.4/mode/properties/properties.min.js"></script>




    
<script src="/js/busuanzi.min.js"></script>

    <script>
        $(document).ready(function () {
            if ($('span[id^="busuanzi_"]').length) {
                initialBusuanzi();
            }
        });
    </script>



<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/default-skin/default-skin.min.css">


<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="//mirrors.sustech.edu.cn/cdnjs/ajax/libs/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="//www.googletagmanager.com/gtag/js?id=UA-69084811-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-69084811-1');
    </script>





<script>
    function initialTyped () {
        var typedTextEl = $('.typed-text');
        if (typedTextEl && typedTextEl.length > 0) {
            var typed = new Typed('.typed-text', {
                strings: ["Stay Hungry. Stay Foolish.", "常与同好争高下，莫与傻子论短长"],
                typeSpeed: 90,
                loop: true,
                loopCount: Infinity,
                backSpeed: 20,
            });
        }
    }

    if ($('.article-header') && $('.article-header').length) {
        $(document).ready(function () {
            initialTyped();
        });
    }
</script>




</html>
